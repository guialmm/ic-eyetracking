{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RFMP52vNQXdC"
      },
      "outputs": [],
      "source": [
        "# Downgrade for√ßado e \"travamento\" para evitar reinstala√ß√£o autom√°tica\n",
        "!pip install numpy==1.24.4 opencv-python==4.8.1.78 opencv-python-headless==4.8.1.78 --force-reinstall --quiet\n",
        "!pip install albumentations==1.3.1 matplotlib scikit-learn tqdm sympy==1.13.1 --quiet\n",
        "\n",
        "# Reinicia o ambiente do Colab para aplicar corretamente a vers√£o do NumPy\n",
        "import os\n",
        "os.kill(os.getpid(), 9)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BfQw0KCLQUIq",
        "outputId": "239a6a50-0555-4257-84ed-d2cac6cede93"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "‚úÖ Sistema configurado\n",
            "üíª Device: cuda\n",
            "üìÇ Base: /content/drive/MyDrive/GuilhermeAlmeida\n",
            "üì∏ 2298 imagens encontradas\n"
          ]
        }
      ],
      "source": [
        "# ========================================================================================\n",
        "# üì¶ IMPORTA√á√ïES E CONFIGURA√á√ÉO - Execute ap√≥s reiniciar o runtime\n",
        "# ========================================================================================\n",
        "\n",
        "# Importa√ß√µes b√°sicas\n",
        "import os\n",
        "import json\n",
        "import time\n",
        "import random\n",
        "import warnings\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "import pickle\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
        "\n",
        "# PyTorch\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "# OpenCV e Albumentations\n",
        "import cv2\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Seeds para reprodutibilidade\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "random.seed(42)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed(42)\n",
        "    torch.cuda.manual_seed_all(42)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "# Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Configura√ß√£o de pastas\n",
        "base_path = \"/content/drive/MyDrive/GuilhermeAlmeida\"\n",
        "folders = [\"fotos\", \"modelos\", \"resultados\", \"resultados/metricas\", \"cache\"]\n",
        "\n",
        "for folder in folders:\n",
        "    os.makedirs(os.path.join(base_path, folder), exist_ok=True)\n",
        "\n",
        "# Hardware\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "print(f\"‚úÖ Sistema configurado\")\n",
        "print(f\"üíª Device: {device}\")\n",
        "print(f\"üìÇ Base: {base_path}\")\n",
        "\n",
        "# Verifica imagens (assumindo que j√° est√£o extra√≠das)\n",
        "fotos_path = os.path.join(base_path, \"fotos\")\n",
        "if os.path.exists(fotos_path):\n",
        "    num_images = len([f for f in os.listdir(fotos_path) if f.lower().endswith(('.jpg', '.jpeg', '.png'))])\n",
        "    print(f\"üì∏ {num_images} imagens encontradas\")\n",
        "else:\n",
        "    print(\"‚ùå Pasta de fotos n√£o encontrada\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ORpBugHKHr0c",
        "outputId": "2c1afd71-e1de-44de-e568-866d0a21067a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4kqpxJqoQdJZ"
      },
      "outputs": [],
      "source": [
        "# ========================================================================================\n",
        "# üìä DATASET AVAN√áADO PARA EYE TRACKING - VERS√ÉO CORRIGIDA\n",
        "# ========================================================================================\n",
        "\n",
        "class AdvancedEyeTrackingDataset(Dataset):\n",
        "    \"\"\"Dataset avan√ßado para eye tracking com pr√©-processamento melhorado e verifica√ß√µes anti-NaN\"\"\"\n",
        "\n",
        "    def __init__(self, images_path, labels_path, transform=None, augment=False, target_size=(224, 224)):\n",
        "        self.images_path = images_path\n",
        "        self.transform = transform\n",
        "        self.augment = augment\n",
        "        self.target_size = target_size\n",
        "\n",
        "        # Carrega as imagens e labels\n",
        "        self.image_files = []\n",
        "        self.labels = []\n",
        "\n",
        "        # CAMINHOS CORRIGIDOS - Procura por imagens na pasta local\n",
        "        print(f\"üîç Procurando imagens em: {images_path}\")\n",
        "        if os.path.exists(images_path):\n",
        "            for file in sorted(os.listdir(images_path)):\n",
        "                if file.endswith(('.jpg', '.jpeg', '.png')):\n",
        "                    full_path = os.path.join(images_path, file)\n",
        "                    self.image_files.append(full_path)\n",
        "\n",
        "        print(f\"üì∏ Encontradas {len(self.image_files)} imagens\")\n",
        "\n",
        "        # Carrega labels se existir arquivo de labels\n",
        "        if os.path.exists(labels_path):\n",
        "            print(f\"üìã Carregando labels de: {labels_path}\")\n",
        "            with open(labels_path, 'r') as f:\n",
        "                labels_data = json.load(f)\n",
        "                # Verifica se √© o formato novo ou antigo\n",
        "                if 'labels' in labels_data:\n",
        "                    self.labels = labels_data['labels']\n",
        "                else:\n",
        "                    self.labels = labels_data\n",
        "\n",
        "                # CORRE√á√ÉO CR√çTICA: Garante que self.labels seja sempre uma lista\n",
        "                if not isinstance(self.labels, list):\n",
        "                    print(f\"‚ö†Ô∏è Labels n√£o √© uma lista, √©: {type(self.labels)}. Convertendo...\")\n",
        "                    if hasattr(self.labels, 'values'):\n",
        "                        # Se for um dict, pega os valores\n",
        "                        self.labels = list(self.labels.values())\n",
        "                    elif hasattr(self.labels, '__iter__'):\n",
        "                        # Se for iter√°vel, converte para lista\n",
        "                        self.labels = list(self.labels)\n",
        "                    else:\n",
        "                        print(\"‚ùå Erro: N√£o foi poss√≠vel converter labels para lista\")\n",
        "                        self.labels = []\n",
        "                \n",
        "                # CORRE√á√ÉO ADICIONAL: Converte todos os labels para float se forem strings\n",
        "                print(\"üîÑ Convertendo labels para formato num√©rico...\")\n",
        "                converted_labels = []\n",
        "                for i, label in enumerate(self.labels):\n",
        "                    try:\n",
        "                        if isinstance(label, (list, tuple)) and len(label) == 2:\n",
        "                            # Converte cada coordenada para float\n",
        "                            x = float(label[0])\n",
        "                            y = float(label[1])\n",
        "                            converted_labels.append([x, y])\n",
        "                        else:\n",
        "                            print(f\"‚ö†Ô∏è Label {i} tem formato inv√°lido: {label}\")\n",
        "                            converted_labels.append([0.5, 0.5])  # Default central\n",
        "                    except (ValueError, TypeError, IndexError) as e:\n",
        "                        print(f\"‚ö†Ô∏è Erro ao converter label {i}: {label} -> {e}\")\n",
        "                        converted_labels.append([0.5, 0.5])  # Default central\n",
        "                \n",
        "                self.labels = converted_labels\n",
        "                print(f\"‚úÖ Labels convertidos: {len(self.labels)} v√°lidos\")\n",
        "                \n",
        "        else:\n",
        "            print(\"‚ö†Ô∏è Arquivo de labels n√£o encontrado. Criando labels dummy...\")\n",
        "            # Gera labels dummy para teste baseados na posi√ß√£o da imagem\n",
        "            self.labels = self._generate_dummy_labels()\n",
        "\n",
        "        # VALIDA√á√ÉO CRUCIAL DOS DADOS\n",
        "        print(\"üîç Validando dados carregados...\")\n",
        "        self._validate_data()\n",
        "\n",
        "        # Verifica consist√™ncia entre imagens e labels\n",
        "        if len(self.image_files) != len(self.labels):\n",
        "            min_length = min(len(self.image_files), len(self.labels))\n",
        "            print(f\"‚ö†Ô∏è Ajustando: {len(self.image_files)} imagens ‚Üí {len(self.labels)} labels. Usando {min_length} amostras.\")\n",
        "            self.image_files = self.image_files[:min_length]\n",
        "            # CORRE√á√ÉO: Garante que self.labels seja uma lista antes do slice\n",
        "            if isinstance(self.labels, list):\n",
        "                self.labels = self.labels[:min_length]\n",
        "            else:\n",
        "                print(f\"‚ö†Ô∏è Erro: self.labels n√£o √© uma lista: {type(self.labels)}\")\n",
        "                # Cria labels dummy se houver problema\n",
        "                self.labels = [[0.5, 0.5] for _ in range(min_length)]\n",
        "\n",
        "        print(f\"‚úÖ Dataset configurado: {len(self.image_files)} amostras\")\n",
        "\n",
        "        # AUGMENTA√á√ïES MAIS CONSERVADORAS para evitar NaN\n",
        "        self.heavy_augmentation = A.Compose([\n",
        "            A.HorizontalFlip(p=0.3),  # Reduzido\n",
        "            A.RandomBrightnessContrast(brightness_limit=0.1, contrast_limit=0.1, p=0.3),  # Mais conservador\n",
        "            A.HueSaturationValue(hue_shift_limit=3, sat_shift_limit=5, val_shift_limit=3, p=0.2),  # Mais conservador\n",
        "            A.GaussNoise(var_limit=(2.0, 10.0), mean=0, p=0.1),  # Reduzido\n",
        "            A.GaussianBlur(blur_limit=1, p=0.1),  # Reduzido\n",
        "            A.ShiftScaleRotate(shift_limit=0.02, scale_limit=0.02, rotate_limit=2, p=0.2),  # Mais conservador\n",
        "            A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "            ToTensorV2()\n",
        "        ])\n",
        "\n",
        "        self.light_augmentation = A.Compose([\n",
        "            A.HorizontalFlip(p=0.2),  # Reduzido\n",
        "            A.RandomBrightnessContrast(brightness_limit=0.05, contrast_limit=0.05, p=0.2),  # Muito conservador\n",
        "            A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "            ToTensorV2()\n",
        "        ])\n",
        "\n",
        "        self.no_augmentation = A.Compose([\n",
        "            A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "            ToTensorV2()\n",
        "        ])\n",
        "\n",
        "        # Estat√≠sticas do dataset\n",
        "        self.compute_dataset_stats()\n",
        "\n",
        "    def _validate_data(self):\n",
        "        \"\"\"Valida dados carregados com corre√ß√£o autom√°tica\"\"\"\n",
        "        invalid_labels = []\n",
        "        for i, label in enumerate(self.labels):\n",
        "            try:\n",
        "                # Garante que √© uma lista/tupla com 2 elementos\n",
        "                if not isinstance(label, (list, tuple)) or len(label) != 2:\n",
        "                    invalid_labels.append(i)\n",
        "                    continue\n",
        "                \n",
        "                # Converte para float se necess√°rio e valida range\n",
        "                x = float(label[0])\n",
        "                y = float(label[1])\n",
        "                \n",
        "                # Verifica se est√° no range v√°lido [0, 1]\n",
        "                if not (0 <= x <= 1 and 0 <= y <= 1):\n",
        "                    invalid_labels.append(i)\n",
        "                    continue\n",
        "                    \n",
        "                # Verifica valores NaN ou Inf\n",
        "                if np.isnan(x) or np.isnan(y) or np.isinf(x) or np.isinf(y):\n",
        "                    invalid_labels.append(i)\n",
        "                    continue\n",
        "                    \n",
        "                # Atualiza o label com valores float validados\n",
        "                self.labels[i] = [float(x), float(y)]\n",
        "                \n",
        "            except (ValueError, TypeError, IndexError) as e:\n",
        "                print(f\"‚ö†Ô∏è Erro ao validar label {i}: {label} -> {e}\")\n",
        "                invalid_labels.append(i)\n",
        "\n",
        "        if invalid_labels:\n",
        "            print(f\"‚ö†Ô∏è Encontrados {len(invalid_labels)} labels inv√°lidos. Corrigindo...\")\n",
        "            for i in invalid_labels:\n",
        "                # Substitui por label v√°lido central\n",
        "                self.labels[i] = [0.5, 0.5]\n",
        "\n",
        "        print(f\"‚úÖ Valida√ß√£o conclu√≠da. {len(self.labels) - len(invalid_labels)} labels v√°lidos.\")\n",
        "\n",
        "    def _generate_dummy_labels(self):\n",
        "        \"\"\"Gera labels dummy baseados na posi√ß√£o da imagem\"\"\"\n",
        "        labels = []\n",
        "        # Pontos de calibra√ß√£o em grade 3x3\n",
        "        calibration_points = [\n",
        "            (0.2, 0.2), (0.5, 0.2), (0.8, 0.2),  # Linha superior\n",
        "            (0.2, 0.5), (0.5, 0.5), (0.8, 0.5),  # Linha central\n",
        "            (0.2, 0.8), (0.5, 0.8), (0.8, 0.8)   # Linha inferior\n",
        "        ]\n",
        "\n",
        "        for i in range(len(self.image_files)):\n",
        "            # Cicla atrav√©s dos pontos de calibra√ß√£o\n",
        "            point_idx = i % len(calibration_points)\n",
        "            gaze_x, gaze_y = calibration_points[point_idx]\n",
        "\n",
        "            # Adiciona pequena varia√ß√£o aleat√≥ria\n",
        "            gaze_x += np.random.normal(0, 0.02)  # Ainda menor\n",
        "            gaze_y += np.random.normal(0, 0.02)\n",
        "\n",
        "            # Garante que est√° no range [0.1, 0.9] para evitar extremos\n",
        "            gaze_x = np.clip(gaze_x, 0.1, 0.9)\n",
        "            gaze_y = np.clip(gaze_y, 0.1, 0.9)\n",
        "\n",
        "            labels.append([float(gaze_x), float(gaze_y)])\n",
        "\n",
        "        return labels\n",
        "\n",
        "    def compute_dataset_stats(self):\n",
        "        \"\"\"Computa estat√≠sticas do dataset para normaliza√ß√£o\"\"\"\n",
        "        print(\"üìä Computando estat√≠sticas do dataset...\")\n",
        "        if len(self.image_files) == 0:\n",
        "            print(\"‚ùå Nenhuma imagem encontrada para calcular estat√≠sticas!\")\n",
        "            return\n",
        "\n",
        "        # Sample de imagens para calcular estat√≠sticas\n",
        "        sample_size = min(100, len(self.image_files))\n",
        "        sample_indices = np.random.choice(len(self.image_files), sample_size, replace=False)\n",
        "\n",
        "        pixel_means = []\n",
        "        pixel_stds = []\n",
        "\n",
        "        for idx in sample_indices:\n",
        "            img_path = self.image_files[idx]\n",
        "            image = cv2.imread(img_path)\n",
        "            if image is not None:\n",
        "                image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "                image = cv2.resize(image, self.target_size)\n",
        "                pixel_means.append(np.mean(image, axis=(0, 1)))\n",
        "                pixel_stds.append(np.std(image, axis=(0, 1)))\n",
        "\n",
        "        if pixel_means:\n",
        "            self.mean = np.mean(pixel_means, axis=0) / 255.0\n",
        "            self.std = np.mean(pixel_stds, axis=0) / 255.0\n",
        "            print(f\"üìä Estat√≠sticas: Mean={self.mean}, Std={self.std}\")\n",
        "        else:\n",
        "            self.mean = [0.485, 0.456, 0.406]\n",
        "            self.std = [0.229, 0.224, 0.225]\n",
        "            print(\"‚ö†Ô∏è Usando estat√≠sticas padr√£o do ImageNet\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_files)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Carrega imagem\n",
        "        img_path = self.image_files[idx]\n",
        "        image = cv2.imread(img_path)\n",
        "\n",
        "        if image is None:\n",
        "            print(f\"‚ùå Erro ao carregar imagem: {img_path}\")\n",
        "            # Retorna imagem dummy e label central\n",
        "            image = np.zeros((224, 224, 3), dtype=np.uint8)\n",
        "            gaze_x, gaze_y = 0.5, 0.5\n",
        "        else:\n",
        "            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "            image = cv2.resize(image, self.target_size)\n",
        "\n",
        "            # Pega o label correspondente\n",
        "            if idx < len(self.labels):\n",
        "                gaze_x, gaze_y = self.labels[idx]\n",
        "            else:\n",
        "                gaze_x, gaze_y = 0.5, 0.5  # Default central\n",
        "\n",
        "        # VERIFICA√á√ïES ANTI-NaN CR√çTICAS\n",
        "        if np.any(np.isnan(image)) or np.any(np.isinf(image)):\n",
        "            print(f\"‚ö†Ô∏è NaN/Inf detectado na imagem {idx}. Substituindo...\")\n",
        "            image = np.clip(image, 0, 255).astype(np.uint8)\n",
        "\n",
        "        if np.isnan(gaze_x) or np.isnan(gaze_y) or np.isinf(gaze_x) or np.isinf(gaze_y):\n",
        "            print(f\"‚ö†Ô∏è NaN/Inf detectado no label {idx}. Usando (0.5, 0.5)\")\n",
        "            gaze_x, gaze_y = 0.5, 0.5\n",
        "\n",
        "        # Clipping final nos labels\n",
        "        gaze_x = np.clip(float(gaze_x), 0.0, 1.0)\n",
        "        gaze_y = np.clip(float(gaze_y), 0.0, 1.0)\n",
        "\n",
        "        # Aplicar transforma√ß√µes\n",
        "        if self.transform:\n",
        "            if self.augment and random.random() < 0.7:\n",
        "                # Usa augmenta√ß√£o mais leve\n",
        "                transformed = self.light_augmentation(image=image)\n",
        "            else:\n",
        "                transformed = self.no_augmentation(image=image)\n",
        "            image = transformed['image']\n",
        "        else:\n",
        "            # Transforma√ß√£o m√≠nima\n",
        "            image = torch.from_numpy(image).permute(2, 0, 1).float() / 255.0\n",
        "            # Normaliza√ß√£o ImageNet\n",
        "            normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "            image = normalize(image)\n",
        "\n",
        "        # VERIFICA√á√ÉO FINAL ANTI-NaN\n",
        "        if torch.any(torch.isnan(image)) or torch.any(torch.isinf(image)):\n",
        "            print(f\"‚ö†Ô∏è NaN/Inf detectado ap√≥s transforma√ß√µes no item {idx}\")\n",
        "            image = torch.zeros(3, self.target_size[0], self.target_size[1])\n",
        "\n",
        "        return image, torch.tensor([gaze_x, gaze_y], dtype=torch.float32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "2xcc_FpeRMkh"
      },
      "outputs": [],
      "source": [
        "# ========================================================================================\n",
        "# üèóÔ∏è COMPONENTES ARQUITETURAIS B√ÅSICOS\n",
        "# ========================================================================================\n",
        "\n",
        "class ResidualBlock(nn.Module):\n",
        "    \"\"\"Bloco residual para melhor fluxo de gradiente\"\"\"\n",
        "\n",
        "    def __init__(self, in_channels, out_channels, stride=1):\n",
        "        super(ResidualBlock, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
        "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_channels != out_channels:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(out_channels)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.bn2(self.conv2(out))\n",
        "        out += self.shortcut(residual)\n",
        "        out = F.relu(out)\n",
        "        return out\n",
        "\n",
        "class SEBlock(nn.Module):\n",
        "    \"\"\"Squeeze-and-Excitation Block para aten√ß√£o de canal\"\"\"\n",
        "\n",
        "    def __init__(self, channel, reduction=16):\n",
        "        super(SEBlock, self).__init__()\n",
        "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(channel, channel // reduction, bias=False),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(channel // reduction, channel, bias=False),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        b, c, _, _ = x.size()\n",
        "        y = self.avg_pool(x).view(b, c)\n",
        "        y = self.fc(y).view(b, c, 1, 1)\n",
        "        return x * y.expand_as(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "UYRK61fkRR2w"
      },
      "outputs": [],
      "source": [
        "class CBAM(nn.Module):\n",
        "    \"\"\"Convolutional Block Attention Module\"\"\"\n",
        "\n",
        "    def __init__(self, channel, reduction=16, spatial_kernel=7):\n",
        "        super(CBAM, self).__init__()\n",
        "        # Channel attention\n",
        "        self.channel_attention = SEBlock(channel, reduction)\n",
        "\n",
        "        # Spatial attention\n",
        "        self.spatial_attention = nn.Sequential(\n",
        "            nn.Conv2d(2, 1, kernel_size=spatial_kernel, stride=1, padding=spatial_kernel // 2, bias=False),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Channel attention\n",
        "        x = self.channel_attention(x)\n",
        "\n",
        "        # Spatial attention\n",
        "        avg_out = torch.mean(x, dim=1, keepdim=True)\n",
        "        max_out, _ = torch.max(x, dim=1, keepdim=True)\n",
        "        attention = torch.cat([avg_out, max_out], dim=1)\n",
        "        attention = self.spatial_attention(attention)\n",
        "\n",
        "        return x * attention"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "TqiZUfvaRVUx"
      },
      "outputs": [],
      "source": [
        "class AdvancedEyeTrackingCNN(nn.Module):\n",
        "    \"\"\"Rede Neural Convolucional Avan√ßada para Eye Tracking\"\"\"\n",
        "\n",
        "    def __init__(self, dropout_rate=0.3, num_classes=2):\n",
        "        super(AdvancedEyeTrackingCNN, self).__init__()\n",
        "\n",
        "        # Entrada: 3x224x224\n",
        "\n",
        "        # Stem - processamento inicial\n",
        "        self.stem = nn.Sequential(\n",
        "            nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "        )\n",
        "\n",
        "        # Blocos residuais com aten√ß√£o\n",
        "        self.layer1 = self._make_layer(64, 64, 2, stride=1)\n",
        "        self.layer2 = self._make_layer(64, 128, 2, stride=2)\n",
        "        self.layer3 = self._make_layer(128, 256, 2, stride=2)\n",
        "        self.layer4 = self._make_layer(256, 512, 2, stride=2)\n",
        "\n",
        "        # Attention modules\n",
        "        self.cbam1 = CBAM(64)\n",
        "        self.cbam2 = CBAM(128)\n",
        "        self.cbam3 = CBAM(256)\n",
        "        self.cbam4 = CBAM(512)\n",
        "\n",
        "        # Global Average Pooling\n",
        "        self.global_avg_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "\n",
        "        # Multi-scale feature extraction\n",
        "        self.multi_scale = nn.ModuleList([\n",
        "            nn.Conv2d(512, 256, kernel_size=1),\n",
        "            nn.Conv2d(512, 256, kernel_size=3, padding=1),\n",
        "            nn.Conv2d(512, 256, kernel_size=5, padding=2),\n",
        "        ])\n",
        "\n",
        "        # Feature fusion\n",
        "        self.feature_fusion = nn.Conv2d(768, 512, kernel_size=1)\n",
        "\n",
        "        # Classificador com m√∫ltiplas cabe√ßas\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(512, 1024),\n",
        "            nn.BatchNorm1d(1024),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(dropout_rate),\n",
        "\n",
        "            nn.Linear(1024, 512),\n",
        "            nn.BatchNorm1d(512),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(dropout_rate),\n",
        "\n",
        "            nn.Linear(512, 256),\n",
        "            nn.BatchNorm1d(256),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(dropout_rate / 2),\n",
        "\n",
        "            nn.Linear(256, num_classes)\n",
        "        )\n",
        "\n",
        "        # Cabe√ßa auxiliar para regulariza√ß√£o\n",
        "        self.aux_classifier = nn.Sequential(\n",
        "            nn.Linear(256, 128),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(dropout_rate),\n",
        "            nn.Linear(128, num_classes)\n",
        "        )\n",
        "\n",
        "        # Inicializa√ß√£o dos pesos\n",
        "        self._initialize_weights()\n",
        "\n",
        "    def _make_layer(self, in_channels, out_channels, blocks, stride):\n",
        "        layers = []\n",
        "        layers.append(ResidualBlock(in_channels, out_channels, stride))\n",
        "        for _ in range(1, blocks):\n",
        "            layers.append(ResidualBlock(out_channels, out_channels))\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def _initialize_weights(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "                if m.bias is not None:\n",
        "                    nn.init.constant_(m.bias, 0)\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                nn.init.constant_(m.weight, 1)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "\n",
        "            elif isinstance(m, nn.Linear):\n",
        "                nn.init.xavier_normal_(m.weight)\n",
        "                if m.bias is not None:\n",
        "                    nn.init.constant_(m.bias, 0)\n",
        "\n",
        "    def forward(self, x, return_aux=False):\n",
        "        # Stem\n",
        "        x = self.stem(x)\n",
        "\n",
        "        # Blocos residuais com aten√ß√£o\n",
        "        x = self.layer1(x)\n",
        "        x = self.cbam1(x)\n",
        "\n",
        "        x = self.layer2(x)\n",
        "        x = self.cbam2(x)\n",
        "\n",
        "        x = self.layer3(x)\n",
        "        x = self.cbam3(x)\n",
        "        aux_features = x  # Para classificador auxiliar\n",
        "\n",
        "        x = self.layer4(x)\n",
        "        x = self.cbam4(x)\n",
        "\n",
        "        # Multi-scale features\n",
        "        ms_features = []\n",
        "        for conv in self.multi_scale:\n",
        "            ms_features.append(conv(x))\n",
        "\n",
        "        # Concatena features multi-scale\n",
        "        x = torch.cat(ms_features, dim=1)\n",
        "        x = self.feature_fusion(x)\n",
        "\n",
        "        # Global pooling\n",
        "        x = self.global_avg_pool(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "\n",
        "        # Classifica√ß√£o principal\n",
        "        main_output = torch.sigmoid(self.classifier(x))\n",
        "\n",
        "        if return_aux and self.training:\n",
        "            # Classificador auxiliar\n",
        "            aux_x = self.global_avg_pool(aux_features)\n",
        "            aux_x = aux_x.view(aux_x.size(0), -1)\n",
        "            aux_output = torch.sigmoid(self.aux_classifier(aux_x))\n",
        "            return main_output, aux_output\n",
        "\n",
        "        return main_output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "dfjZI1RvRYBi"
      },
      "outputs": [],
      "source": [
        "class EnsembleModel(nn.Module):\n",
        "    \"\"\"Ensemble de m√∫ltiplos modelos para melhor performance\"\"\"\n",
        "\n",
        "    def __init__(self, num_models=3):\n",
        "        super(EnsembleModel, self).__init__()\n",
        "        self.models = nn.ModuleList([\n",
        "            AdvancedEyeTrackingCNN(dropout_rate=0.2 + i * 0.1)\n",
        "            for i in range(num_models)\n",
        "        ])\n",
        "        self.num_models = num_models\n",
        "\n",
        "    def forward(self, x):\n",
        "        outputs = []\n",
        "        for model in self.models:\n",
        "            outputs.append(model(x))\n",
        "\n",
        "        # M√©dia ponderada dos outputs\n",
        "        weights = torch.softmax(torch.randn(self.num_models), dim=0).to(x.device)\n",
        "        ensemble_output = sum(w * out for w, out in zip(weights, outputs))\n",
        "\n",
        "        return ensemble_output\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "R3w2Dm8_RZ6R"
      },
      "outputs": [],
      "source": [
        "class MetricsHandler:\n",
        "    \"\"\"Classe auxiliar para manipular m√©tricas e gr√°ficos\"\"\"\n",
        "\n",
        "    def __init__(self, results_path='/content/drive/MyDrive/GuilhermeAlmeida/resultados/'):\n",
        "        self.results_path = results_path\n",
        "        os.makedirs(os.path.join(self.results_path, 'metricas'), exist_ok=True)\n",
        "\n",
        "    def save_advanced_metrics(self, metrics_data, filename):\n",
        "        \"\"\"Salva m√©tricas avan√ßadas no Google Drive\"\"\"\n",
        "        metrics_path = os.path.join(self.results_path, 'metricas', filename)\n",
        "        os.makedirs(os.path.dirname(metrics_path), exist_ok=True)\n",
        "\n",
        "        with open(metrics_path, 'w') as f:\n",
        "            json.dump(metrics_data, f, indent=2)\n",
        "\n",
        "        print(f\"üìä M√©tricas salvas no Google Drive: {metrics_path}\")\n",
        "\n",
        "    def plot_training_curves(self, train_losses, val_losses, train_accs, val_accs, learning_rates, save_path=None):\n",
        "        \"\"\"Plota curvas de treinamento avan√ßadas e salva no Google Drive\"\"\"\n",
        "        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(16, 12))\n",
        "\n",
        "        epochs = range(1, len(train_losses) + 1)\n",
        "\n",
        "        # Plot das perdas\n",
        "        ax1.plot(epochs, train_losses, 'b-', label='Perda Treino', linewidth=2)\n",
        "        ax1.plot(epochs, val_losses, 'r-', label='Perda Valida√ß√£o', linewidth=2)\n",
        "        ax1.set_title('Curvas de Perda', fontsize=14, fontweight='bold')\n",
        "        ax1.set_xlabel('√âpoca')\n",
        "        ax1.set_ylabel('Perda Combinada')\n",
        "        ax1.legend()\n",
        "        ax1.grid(True, alpha=0.3)\n",
        "\n",
        "        # Plot das acur√°cias\n",
        "        ax2.plot(epochs, train_accs, 'b-', label='Erro Treino', linewidth=2)\n",
        "        ax2.plot(epochs, val_accs, 'r-', label='Erro Valida√ß√£o', linewidth=2)\n",
        "        ax2.set_title('Erro Euclidiano', fontsize=14, fontweight='bold')\n",
        "        ax2.set_xlabel('√âpoca')\n",
        "        ax2.set_ylabel('Dist√¢ncia Euclidiana')\n",
        "        ax2.legend()\n",
        "        ax2.grid(True, alpha=0.3)\n",
        "\n",
        "        # Learning rate\n",
        "        ax3.plot(epochs, learning_rates, 'g-', linewidth=2)\n",
        "        ax3.set_title('Learning Rate', fontsize=14, fontweight='bold')\n",
        "        ax3.set_xlabel('√âpoca')\n",
        "        ax3.set_ylabel('Learning Rate')\n",
        "        ax3.set_yscale('log')\n",
        "        ax3.grid(True, alpha=0.3)\n",
        "\n",
        "        # Overfitting detection\n",
        "        if len(train_losses) > 10:\n",
        "            train_smooth = np.convolve(train_losses, np.ones(5) / 5, mode='valid')\n",
        "            val_smooth = np.convolve(val_losses, np.ones(5) / 5, mode='valid')\n",
        "            gap = val_smooth - train_smooth\n",
        "            ax4.plot(range(3, len(gap) + 3), gap, 'purple', linewidth=2)\n",
        "            ax4.axhline(y=0, color='black', linestyle='--', alpha=0.5)\n",
        "            ax4.set_title('Gap Treino-Valida√ß√£o (Overfitting)', fontsize=14, fontweight='bold')\n",
        "            ax4.set_xlabel('√âpoca')\n",
        "            ax4.set_ylabel('Val Loss - Train Loss')\n",
        "            ax4.grid(True, alpha=0.3)\n",
        "\n",
        "        plt.tight_layout()\n",
        "\n",
        "        if save_path:\n",
        "            # Salva no Google Drive\n",
        "            full_save_path = os.path.join(self.results_path, save_path)\n",
        "            os.makedirs(os.path.dirname(full_save_path), exist_ok=True)\n",
        "            plt.savefig(full_save_path, dpi=300, bbox_inches='tight')\n",
        "            print(f\"üìà Gr√°fico salvo no Google Drive: {full_save_path}\")\n",
        "\n",
        "        plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "DZO7Q2jtSqKx"
      },
      "outputs": [],
      "source": [
        "def create_smart_labels_from_calibration():\n",
        "    \"\"\"Cria labels inteligentes baseados nos dados de calibra√ß√£o e padr√µes de captura\"\"\"\n",
        "    # CAMINHOS DO GOOGLE DRIVE\n",
        "    calibration_file = '/content/drive/MyDrive/GuilhermeAlmeida/cache/calibragem.json'\n",
        "\n",
        "    # Se n√£o existe calibra√ß√£o, cria dados padr√£o\n",
        "    calibration_data = {}\n",
        "    if os.path.exists(calibration_file):\n",
        "        with open(calibration_file, 'r') as f:\n",
        "            calibration_data = json.load(f)\n",
        "        print(\"‚úÖ Dados de calibra√ß√£o carregados do Google Drive\")\n",
        "    else:\n",
        "        print(\"‚ö†Ô∏è Arquivo de calibra√ß√£o n√£o encontrado no Google Drive. Usando padr√µes.\")\n",
        "\n",
        "    # CAMINHO CORRIGIDO - Google Drive\n",
        "    captured_faces_path = \"/content/drive/MyDrive/GuilhermeAlmeida/fotos\"\n",
        "    if not os.path.exists(captured_faces_path):\n",
        "        print(f\"‚ùå Pasta {captured_faces_path} n√£o encontrada no Google Drive.\")\n",
        "        print(\"üìÅ Certifique-se de que o Google Drive est√° montado e que a pasta existe\")\n",
        "        return None\n",
        "\n",
        "    # Coleta todas as imagens do Google Drive\n",
        "    image_files = []\n",
        "    for file in sorted(os.listdir(captured_faces_path)):\n",
        "        if file.lower().endswith(('.jpg', '.jpeg', '.png')):\n",
        "            # Usa caminho relativo para compatibilidade\n",
        "            image_files.append(file)\n",
        "\n",
        "    if not image_files:\n",
        "        print(\"‚ùå Nenhuma imagem encontrada na pasta do Google Drive.\")\n",
        "        return None\n",
        "\n",
        "    print(f\"üì∏ Encontradas {len(image_files)} imagens para labeling no Google Drive\")\n",
        "\n",
        "    # Pontos de calibra√ß√£o t√≠picos em grid 3x3\n",
        "    calibration_points = [\n",
        "        (0.1, 0.1), (0.5, 0.1), (0.9, 0.1),  # Linha superior\n",
        "        (0.1, 0.5), (0.5, 0.5), (0.9, 0.5),  # Linha central\n",
        "        (0.1, 0.9), (0.5, 0.9), (0.9, 0.9)   # Linha inferior\n",
        "    ]\n",
        "\n",
        "    # Cria labels para todas as imagens\n",
        "    labels = []\n",
        "    for i, img_file in enumerate(image_files):\n",
        "        # Usa padr√£o circular atrav√©s dos pontos de calibra√ß√£o\n",
        "        point_idx = i % len(calibration_points)\n",
        "        gaze_x, gaze_y = calibration_points[point_idx]\n",
        "\n",
        "        # Adiciona varia√ß√£o pequena para simular movimento natural\n",
        "        gaze_x += np.random.normal(0, 0.03)  # Varia√ß√£o menor\n",
        "        gaze_y += np.random.normal(0, 0.03)\n",
        "\n",
        "        # Garante que est√° no range [0, 1]\n",
        "        gaze_x = np.clip(gaze_x, 0.05, 0.95)  # Evita bordas extremas\n",
        "        gaze_y = np.clip(gaze_y, 0.05, 0.95)\n",
        "\n",
        "        labels.append([float(gaze_x), float(gaze_y)])  # Assegura tipo float\n",
        "\n",
        "    # Salva dados organizados no Google Drive\n",
        "    dataset_info = {\n",
        "        'images': image_files,  # Apenas nomes dos arquivos\n",
        "        'labels': labels,\n",
        "        'calibration_data': calibration_data,\n",
        "        'num_samples': len(labels),\n",
        "        'created_at': time.time(),\n",
        "        'images_path': captured_faces_path  # Caminho base do Google Drive\n",
        "    }\n",
        "\n",
        "    # Salva no Google Drive\n",
        "    labels_path = '/content/drive/MyDrive/GuilhermeAlmeida/smart_labels.json'\n",
        "\n",
        "    # Cria diret√≥rio se n√£o existir\n",
        "    os.makedirs(os.path.dirname(labels_path), exist_ok=True)\n",
        "\n",
        "    with open(labels_path, 'w') as f:\n",
        "        json.dump(dataset_info, f, indent=2)\n",
        "\n",
        "    print(f\"‚úÖ Labels inteligentes criados para {len(labels)} imagens\")\n",
        "    print(f\"üìÇ Salvo no Google Drive: {labels_path}\")\n",
        "\n",
        "    # VALIDA√á√ÉO DOS LABELS\n",
        "    print(\"üîç Validando labels criados...\")\n",
        "    for i, label in enumerate(labels[:5]):  # Testa primeiros 5\n",
        "        if not (0 <= label[0] <= 1 and 0 <= label[1] <= 1):\n",
        "            print(f\"‚ö†Ô∏è Label {i} fora do range: {label}\")\n",
        "        if np.isnan(label[0]) or np.isnan(label[1]):\n",
        "            print(f\"‚ùå Label {i} cont√©m NaN: {label}\")\n",
        "    print(\"‚úÖ Valida√ß√£o de labels conclu√≠da\")\n",
        "\n",
        "    return labels_path\n",
        "\n",
        "def load_smart_dataset(dataset_path):\n",
        "    \"\"\"Carrega dataset com informa√ß√µes inteligentes do Google Drive\"\"\"\n",
        "    with open(dataset_path, 'r') as f:\n",
        "        dataset_info = json.load(f)\n",
        "\n",
        "    # Reconstr√≥i caminhos completos do Google Drive\n",
        "    images_base_path = dataset_info.get('images_path', '/content/drive/MyDrive/GuilhermeAlmeida/fotos')\n",
        "    image_files = dataset_info['images']\n",
        "\n",
        "    # Se s√£o apenas nomes, reconstr√≥i caminhos completos do Google Drive\n",
        "    if not os.path.isabs(image_files[0]):\n",
        "        image_files = [os.path.join(images_base_path, img) for img in image_files]\n",
        "\n",
        "    return image_files, dataset_info['labels']\n",
        "\n",
        "def load_advanced_trained_model(model_path='/content/drive/MyDrive/GuilhermeAlmeida/modelos/melhor_modelo_eyetracking_CORRIGIDO.pth'):\n",
        "    \"\"\"Carrega um modelo avan√ßado treinado do Google Drive\"\"\"\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    # Carrega checkpoint\n",
        "    checkpoint = torch.load(model_path, map_location=device)\n",
        "\n",
        "    # Cria modelo\n",
        "    model = AdvancedEyeTrackingCNN()\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    print(f\"‚úÖ Modelo carregado do epoch {checkpoint.get('epoch', 'desconhecido')}\")\n",
        "    print(f\"üéØ Melhor acur√°cia: {checkpoint.get('best_metrics', {}).get('val_accuracy', 'N/A')}\")\n",
        "\n",
        "    return model, device\n",
        "\n",
        "def predict_gaze_advanced(model, image, device):\n",
        "    \"\"\"Prediz coordenadas de gaze com modelo avan√ßado\"\"\"\n",
        "    # Pr√©-processamento\n",
        "    if len(image.shape) == 3:\n",
        "        image = cv2.resize(image, (224, 224), interpolation=cv2.INTER_LANCZOS4)\n",
        "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "        # Normaliza√ß√£o\n",
        "        mean = np.array([0.485, 0.456, 0.406])\n",
        "        std = np.array([0.229, 0.224, 0.225])\n",
        "        image = image.astype(np.float32) / 255.0\n",
        "        image = (image - mean) / std\n",
        "\n",
        "        # Para tensor\n",
        "        image = torch.from_numpy(image).permute(2, 0, 1).unsqueeze(0)\n",
        "\n",
        "    image = image.to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        prediction = model(image)\n",
        "        gaze_x, gaze_y = prediction[0].cpu().numpy()\n",
        "\n",
        "    return float(gaze_x), float(gaze_y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "p1Q4MekjBVdq"
      },
      "outputs": [],
      "source": [
        "# ========================================================================================\n",
        "# üîß VERS√ÉO CORRIGIDA DO TRAINER - LEARNING RATE FIXO E M√âTRICAS AJUSTADAS\n",
        "# ========================================================================================\n",
        "\n",
        "class FixedLRAdvancedEyeTrackingTrainer:\n",
        "    \"\"\"Classe corrigida para treinamento com LR fixo e m√©tricas mais realistas\"\"\"\n",
        "\n",
        "    def __init__(self, model, device, use_amp=False, save_path='/content/drive/MyDrive/GuilhermeAlmeida/modelos/', results_path='/content/drive/MyDrive/GuilhermeAlmeida/resultados/'):\n",
        "        self.model = model.to(device)\n",
        "        self.device = device\n",
        "        self.use_amp = use_amp\n",
        "        self.scaler = torch.cuda.amp.GradScaler(enabled=use_amp)\n",
        "\n",
        "        # Caminhos do Google Drive\n",
        "        self.save_path = save_path\n",
        "        self.results_path = results_path\n",
        "\n",
        "        # Cria diret√≥rios necess√°rios no Google Drive\n",
        "        os.makedirs(self.save_path, exist_ok=True)\n",
        "        os.makedirs(self.results_path, exist_ok=True)\n",
        "        os.makedirs(os.path.join(self.results_path, 'metricas'), exist_ok=True)\n",
        "\n",
        "        # M√©tricas de treinamento\n",
        "        self.train_losses = []\n",
        "        self.val_losses = []\n",
        "        self.train_accuracies = []\n",
        "        self.val_accuracies = []\n",
        "        self.learning_rates = []\n",
        "        self.best_metrics = {\n",
        "            'val_loss': float('inf'),\n",
        "            'val_accuracy': float('inf'),\n",
        "            'epoch': 0\n",
        "        }\n",
        "\n",
        "    def combined_loss(self, predictions, targets, aux_predictions=None):\n",
        "        \"\"\"Loss combinada com verifica√ß√£o de NaN melhorada\"\"\"\n",
        "        # Verifica√ß√£o rigorosa de entrada\n",
        "        if torch.isnan(predictions).any() or torch.isinf(predictions).any():\n",
        "            print(\"‚ùå ERRO: Predi√ß√µes cont√™m NaN/Inf!\")\n",
        "        self.scheduler.step()\n",
        "        return torch.tensor(1.0, device=predictions.device, requires_grad=True)\n",
        "\n",
        "        if torch.isnan(targets).any() or torch.isinf(targets).any():\n",
        "            print(\"‚ùå ERRO: Targets cont√™m NaN/Inf!\")\n",
        "            return torch.tensor(1.0, device=predictions.device, requires_grad=True)\n",
        "\n",
        "        # Clamp das predi√ß√µes para range v√°lido\n",
        "        predictions = torch.clamp(predictions, 0.001, 0.999)\n",
        "        targets = torch.clamp(targets, 0.001, 0.999)\n",
        "\n",
        "        # MSE Loss b√°sico\n",
        "        mse_loss = F.mse_loss(predictions, targets)\n",
        "\n",
        "        # Verifica√ß√£o do resultado\n",
        "        if torch.isnan(mse_loss) or torch.isinf(mse_loss):\n",
        "            print(\"‚ùå ERRO: MSE Loss inv√°lido!\")\n",
        "            return torch.tensor(0.5, device=predictions.device, requires_grad=True)\n",
        "\n",
        "        # Adiciona pequena regulariza√ß√£o para estabilidade\n",
        "        l2_reg = 0.0001 * torch.mean(predictions ** 2)\n",
        "        total_loss = mse_loss + l2_reg\n",
        "\n",
        "        # Verifica√ß√£o final\n",
        "        if torch.isnan(total_loss) or torch.isinf(total_loss):\n",
        "            return torch.tensor(0.5, device=predictions.device, requires_grad=True)\n",
        "\n",
        "        return total_loss\n",
        "\n",
        "    def calculate_improved_metrics(self, predictions, targets):\n",
        "        \"\"\"Calcula m√©tricas melhoradas com thresholds mais realistas\"\"\"\n",
        "        if torch.isnan(predictions).any() or torch.isnan(targets).any():\n",
        "            return {\n",
        "                'euclidean_distance': float('inf'),\n",
        "                'angular_error': float('inf'),\n",
        "                'accuracy_10': 0.0,\n",
        "                'accuracy_20': 0.0,\n",
        "                'accuracy_30': 0.0\n",
        "            }\n",
        "\n",
        "        predictions = torch.clamp(predictions, 0.0, 1.0)\n",
        "        targets = torch.clamp(targets, 0.0, 1.0)\n",
        "        euclidean_dist = torch.sqrt(torch.sum((predictions - targets) ** 2, dim=1))\n",
        "        mean_euclidean = torch.mean(euclidean_dist).item()\n",
        "\n",
        "        try:\n",
        "            pred_norm = F.normalize(predictions + 1e-8, p=2, dim=1)\n",
        "            target_norm = F.normalize(targets + 1e-8, p=2, dim=1)\n",
        "            cosine_sim = torch.sum(pred_norm * target_norm, dim=1)\n",
        "            cosine_sim = torch.clamp(cosine_sim, -0.999, 0.999)\n",
        "            angular_error = torch.acos(cosine_sim)\n",
        "            mean_angular = torch.mean(angular_error).item()\n",
        "        except:\n",
        "            mean_angular = float('inf')\n",
        "\n",
        "        accuracy_10 = torch.mean((euclidean_dist < 0.1).float()).item()\n",
        "        accuracy_20 = torch.mean((euclidean_dist < 0.2).float()).item()\n",
        "        accuracy_30 = torch.mean((euclidean_dist < 0.3).float()).item()\n",
        "\n",
        "        return {\n",
        "            'euclidean_distance': mean_euclidean,\n",
        "            'angular_error': mean_angular,\n",
        "            'accuracy_10': accuracy_10,\n",
        "            'accuracy_20': accuracy_20,\n",
        "            'accuracy_30': accuracy_30\n",
        "        }\n",
        "\n",
        "    def train_epoch(self, train_loader, optimizer):\n",
        "        self.model.train()\n",
        "        total_loss = 0\n",
        "        total_metrics = {'euclidean_distance': 0, 'angular_error': 0, 'accuracy_10': 0, 'accuracy_20': 0, 'accuracy_30': 0}\n",
        "        valid_batches = 0\n",
        "        skipped_batches = 0\n",
        "\n",
        "        pbar = tqdm(train_loader, desc=\"Training\", leave=False)\n",
        "        for batch_idx, (data, target) in enumerate(pbar):\n",
        "\n",
        "            # Move para device\n",
        "            data, target = data.to(self.device), target.to(self.device)\n",
        "\n",
        "            # Verifica√ß√£o de dados de entrada\n",
        "            if torch.isnan(data).any() or torch.isinf(data).any():\n",
        "                skipped_batches += 1\n",
        "                continue\n",
        "\n",
        "            if torch.isnan(target).any() or torch.isinf(target).any():\n",
        "                skipped_batches += 1\n",
        "                continue\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            try:\n",
        "                if self.use_amp:\n",
        "                    with torch.cuda.amp.autocast():\n",
        "                        output = self.model(data)\n",
        "\n",
        "                        # Verifica√ß√£o da sa√≠da\n",
        "                        if torch.isnan(output).any() or torch.isinf(output).any():\n",
        "                            skipped_batches += 1\n",
        "                            continue\n",
        "\n",
        "                        loss = self.combined_loss(output, target)\n",
        "\n",
        "                    # Verifica√ß√£o do loss antes do backward\n",
        "                    if torch.isnan(loss) or torch.isinf(loss) or loss.item() == float('inf'):\n",
        "                        skipped_batches += 1\n",
        "                        continue\n",
        "\n",
        "                    # Backward pass\n",
        "                    self.scaler.scale(loss).backward()\n",
        "\n",
        "                    # Verifica gradientes\n",
        "                    grad_norm = 0\n",
        "                    for param in self.model.parameters():\n",
        "                        if param.grad is not None:\n",
        "                            if torch.isnan(param.grad).any() or torch.isinf(param.grad).any():\n",
        "                                skipped_batches += 1\n",
        "                                break\n",
        "                            grad_norm += param.grad.data.norm(2).item() ** 2\n",
        "                    else:  # S√≥ executa se n√£o houve break\n",
        "                        grad_norm = grad_norm ** 0.5\n",
        "\n",
        "                        if grad_norm > 10.0:\n",
        "                            # Apenas conta como skip sem print detalhado\n",
        "                            pass\n",
        "\n",
        "                        self.scaler.unscale_(optimizer)\n",
        "                        torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)\n",
        "                        self.scaler.step(optimizer)\n",
        "                        self.scaler.update()\n",
        "\n",
        "                        # Calcula m√©tricas\n",
        "                        with torch.no_grad():\n",
        "                            metrics = self.calculate_improved_metrics(output, target)\n",
        "                            if not any(np.isinf(v) or np.isnan(v) for v in metrics.values()):\n",
        "                                for key in total_metrics:\n",
        "                                    total_metrics[key] += metrics[key]\n",
        "                                total_loss += loss.item()\n",
        "                                valid_batches += 1\n",
        "                            else:\n",
        "                                skipped_batches += 1\n",
        "\n",
        "                else:\n",
        "                    # Sem AMP\n",
        "                    output = self.model(data)\n",
        "\n",
        "                    if torch.isnan(output).any() or torch.isinf(output).any():\n",
        "                        skipped_batches += 1\n",
        "                        continue\n",
        "\n",
        "                    loss = self.combined_loss(output, target)\n",
        "\n",
        "                    if torch.isnan(loss) or torch.isinf(loss) or loss.item() == float('inf'):\n",
        "                        skipped_batches += 1\n",
        "                        continue\n",
        "\n",
        "                    loss.backward()\n",
        "\n",
        "                    # Verifica gradientes\n",
        "                    grad_norm = torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)\n",
        "                    if torch.isnan(grad_norm) or grad_norm > 10.0:\n",
        "                        skipped_batches += 1\n",
        "                        continue\n",
        "\n",
        "                    optimizer.step()\n",
        "\n",
        "                    # Calcula m√©tricas\n",
        "                    with torch.no_grad():\n",
        "                        metrics = self.calculate_improved_metrics(output, target)\n",
        "                        if not any(np.isinf(v) or np.isnan(v) for v in metrics.values()):\n",
        "                            for key in total_metrics:\n",
        "                                total_metrics[key] += metrics[key]\n",
        "                            total_loss += loss.item()\n",
        "                            valid_batches += 1\n",
        "                        else:\n",
        "                            skipped_batches += 1\n",
        "\n",
        "                # Atualiza progress bar\n",
        "                if valid_batches > 0:\n",
        "                    current_loss = total_loss / valid_batches\n",
        "                    current_eucl = total_metrics['euclidean_distance'] / valid_batches\n",
        "                    current_acc20 = total_metrics['accuracy_20'] / valid_batches\n",
        "\n",
        "                    pbar.set_postfix({\n",
        "                        'Loss': f\"{current_loss:.4f}\",\n",
        "                        'Eucl': f\"{current_eucl:.4f}\",\n",
        "                        'Acc20': f\"{current_acc20:.3f}\",\n",
        "                        'Valid': f\"{valid_batches}/{batch_idx+1}\",\n",
        "                        'Skip': f\"{skipped_batches}\"\n",
        "                    })\n",
        "\n",
        "            except Exception as e:\n",
        "                skipped_batches += 1\n",
        "                continue\n",
        "\n",
        "        # Resultados finais da √©poca (simplificado)\n",
        "        if valid_batches > 0:\n",
        "            avg_loss = total_loss / valid_batches\n",
        "            for key in total_metrics:\n",
        "                total_metrics[key] /= valid_batches\n",
        "        else:\n",
        "            avg_loss = float('inf')\n",
        "            for key in total_metrics:\n",
        "                total_metrics[key] = float('inf')\n",
        "\n",
        "        return avg_loss, total_metrics\n",
        "\n",
        "    def validate_epoch(self, val_loader):\n",
        "        self.model.eval()\n",
        "        total_loss = 0\n",
        "        total_metrics = {'euclidean_distance': 0, 'angular_error': 0, 'accuracy_10': 0, 'accuracy_20': 0, 'accuracy_30': 0}\n",
        "        valid_batches = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for data, target in tqdm(val_loader, desc=\"Validating\", leave=False):\n",
        "                data, target = data.to(self.device), target.to(self.device)\n",
        "                if torch.isnan(data).any() or torch.isnan(target).any():\n",
        "                    continue\n",
        "\n",
        "                try:\n",
        "                    if self.use_amp:\n",
        "                        with torch.cuda.amp.autocast():\n",
        "                            output = self.model(data)\n",
        "                            loss = self.combined_loss(output, target)\n",
        "                    else:\n",
        "                        output = self.model(data)\n",
        "                        loss = self.combined_loss(output, target)\n",
        "\n",
        "                    if torch.isnan(loss) or torch.isinf(loss):\n",
        "                        continue\n",
        "\n",
        "                    metrics = self.calculate_improved_metrics(output, target)\n",
        "                    if not any(np.isinf(v) or np.isnan(v) for v in metrics.values()):\n",
        "                        for key in total_metrics:\n",
        "                            total_metrics[key] += metrics[key]\n",
        "                        total_loss += loss.item()\n",
        "                        valid_batches += 1\n",
        "                except Exception:\n",
        "                    continue\n",
        "\n",
        "        if valid_batches > 0:\n",
        "            avg_loss = total_loss / valid_batches\n",
        "            for key in total_metrics:\n",
        "                total_metrics[key] /= valid_batches\n",
        "        else:\n",
        "            avg_loss = float('inf')\n",
        "            for key in total_metrics:\n",
        "                total_metrics[key] = float('inf')\n",
        "\n",
        "        return avg_loss, total_metrics\n",
        "\n",
        "    def train(self, train_loader, val_loader, epochs=100, learning_rate=0.0001, use_fixed_lr=True):\n",
        "        optimizer = optim.Adam(self.model.parameters(), lr=learning_rate, weight_decay=1e-5, eps=1e-8)\n",
        "\n",
        "        scheduler = None\n",
        "        if not use_fixed_lr:\n",
        "            scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=20, min_lr=1e-6, verbose=True)\n",
        "\n",
        "        patience = 1000\n",
        "        patience_counter = 0\n",
        "\n",
        "        print(f\"Iniciando treinamento CORRIGIDO por {epochs} √©pocas...\")\n",
        "        print(f\"Device: {self.device}\")\n",
        "        print(f\"Learning Rate: {learning_rate} (FIXO: {use_fixed_lr})\")\n",
        "        print(f\"Mixed Precision: {self.use_amp}\")\n",
        "        print(f\"Par√¢metros do modelo: {sum(p.numel() for p in self.model.parameters()):,}\")\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "            print(f\"\\n{'=' * 20} √âpoca {epoch + 1}/{epochs} {'=' * 20}\")\n",
        "            train_loss, train_metrics = self.train_epoch(train_loader, optimizer)\n",
        "            val_loss, val_metrics = self.validate_epoch(val_loader)\n",
        "\n",
        "            if scheduler:\n",
        "                scheduler.step(val_loss)\n",
        "\n",
        "            self.train_losses.append(train_loss)\n",
        "            self.val_losses.append(val_loss)\n",
        "            self.train_accuracies.append(train_metrics['euclidean_distance'])\n",
        "            self.val_accuracies.append(val_metrics['euclidean_distance'])\n",
        "\n",
        "            current_lr = optimizer.param_groups[0]['lr']\n",
        "            self.learning_rates.append(current_lr)\n",
        "\n",
        "            print(f\"Train - Loss: {train_loss:.4f}, Eucl: {train_metrics['euclidean_distance']:.4f}\")\n",
        "            print(f\"      - Acc@10%: {train_metrics['accuracy_10']:.3f}, Acc@20%: {train_metrics['accuracy_20']:.3f}, Acc@30%: {train_metrics['accuracy_30']:.3f}\")\n",
        "            print(f\"Val   - Loss: {val_loss:.4f}, Eucl: {val_metrics['euclidean_distance']:.4f}\")\n",
        "            print(f\"      - Acc@10%: {val_metrics['accuracy_10']:.3f}, Acc@20%: {val_metrics['accuracy_20']:.3f}, Acc@30%: {val_metrics['accuracy_30']:.3f}\")\n",
        "            print(f\"Learning Rate: {current_lr:.2e}\")\n",
        "\n",
        "            # Plota gr√°ficos a cada 10 √©pocas\n",
        "            if (epoch + 1) % 10 == 0:\n",
        "                try:\n",
        "                    metrics_handler = MetricsHandler(self.results_path)\n",
        "                    metrics_handler.plot_training_curves(\n",
        "                        self.train_losses,\n",
        "                        self.val_losses,\n",
        "                        self.train_accuracies,\n",
        "                        self.val_accuracies,\n",
        "                        self.learning_rates,\n",
        "                        f'curvas_epoca_{epoch + 1}.png'\n",
        "                    )\n",
        "                    print(f\"üìà Gr√°ficos salvos para √©poca {epoch + 1}\")\n",
        "                except Exception as e:\n",
        "                    print(f\"‚ö†Ô∏è Erro ao salvar gr√°ficos: {e}\")\n",
        "\n",
        "            if np.isinf(train_loss) or np.isnan(train_loss):\n",
        "                print(\"‚ùå ERRO: Loss de treino inv√°lido! Parando treinamento...\")\n",
        "                break\n",
        "\n",
        "            if val_metrics['accuracy_20'] > self.best_metrics.get('best_acc_20', 0):\n",
        "                self.best_metrics.update({\n",
        "                    'val_loss': val_loss,\n",
        "                    'val_accuracy': val_metrics['euclidean_distance'],\n",
        "                    'best_acc_20': val_metrics['accuracy_20'],\n",
        "                    'epoch': epoch + 1\n",
        "                })\n",
        "                patience_counter = 0\n",
        "                best_model_path = os.path.join(self.save_path, 'melhor_modelo_eyetracking_CORRIGIDO.pth')\n",
        "                torch.save({\n",
        "                    'model_state_dict': self.model.state_dict(),\n",
        "                    'optimizer_state_dict': optimizer.state_dict(),\n",
        "                    'epoch': epoch + 1,\n",
        "                    'best_metrics': self.best_metrics,\n",
        "                    'train_losses': self.train_losses,\n",
        "                    'val_losses': self.val_losses\n",
        "                }, best_model_path)\n",
        "                print(f\"‚úì Novo melhor modelo salvo! Acc@20%: {val_metrics['accuracy_20']:.3f}\")\n",
        "            else:\n",
        "                patience_counter += 1\n",
        "\n",
        "            if patience_counter >= patience:\n",
        "                print(f\"Early stopping ap√≥s {epoch + 1} √©pocas\")\n",
        "                break\n",
        "\n",
        "        print(f\"\\n{'=' * 60}\")\n",
        "        print(\"Treinamento CORRIGIDO conclu√≠do!\")\n",
        "        print(f\"Melhor √©poca: {self.best_metrics['epoch']}\")\n",
        "        best_acc_20 = self.best_metrics.get(\"best_acc_20\", None)\n",
        "        if best_acc_20 is not None:\n",
        "            print(f\"Melhor Acc@20%: {best_acc_20:.3f}\")\n",
        "        else:\n",
        "            print(\"Melhor Acc@20%: N/A\")\n",
        "\n",
        "        return self.train_losses, self.val_losses, self.train_accuracies, self.val_accuracies\n",
        "\n",
        "        # Scheduler: reduz o learning rate a cada 100 √©pocas (gamma = 0.1)\n",
        "        self.scheduler = torch.optim.lr_scheduler.StepLR(self.optimizer, step_size=100, gamma=0.1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "ewOokNU4t3pG"
      },
      "outputs": [],
      "source": [
        "# ========================================================================================\n",
        "# üîß FUN√á√ïES AUXILIARES PARA VERIFICA√á√ÉO E DEBUGGING\n",
        "# ========================================================================================\n",
        "\n",
        "def verify_model_initialization(model, device):\n",
        "    \"\"\"Verifica se o modelo est√° inicializado corretamente\"\"\"\n",
        "    print(\"üîç Verificando inicializa√ß√£o do modelo...\")\n",
        "\n",
        "    # Verifica se h√° par√¢metros com valores inv√°lidos\n",
        "    invalid_params = 0\n",
        "    total_params = 0\n",
        "\n",
        "    for name, param in model.named_parameters():\n",
        "        total_params += param.numel()\n",
        "        if torch.isnan(param).any() or torch.isinf(param).any():\n",
        "            print(f\"‚ùå Par√¢metro inv√°lido encontrado: {name}\")\n",
        "            invalid_params += param.numel()\n",
        "            # Reinicializa o par√¢metro problem√°tico\n",
        "            if 'weight' in name:\n",
        "                nn.init.xavier_normal_(param)\n",
        "            elif 'bias' in name:\n",
        "                nn.init.constant_(param, 0)\n",
        "\n",
        "    if invalid_params > 0:\n",
        "        print(f\"‚ö†Ô∏è {invalid_params}/{total_params} par√¢metros foram reinicializados\")\n",
        "    else:\n",
        "        print(\"‚úÖ Todos os par√¢metros est√£o v√°lidos\")\n",
        "\n",
        "    # Teste com entrada sint√©tica\n",
        "    print(\"üß™ Testando forward pass...\")\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        test_input = torch.randn(2, 3, 224, 224).to(device)\n",
        "        try:\n",
        "            test_output = model(test_input)\n",
        "            if torch.isnan(test_output).any() or torch.isinf(test_output).any():\n",
        "                print(\"‚ùå Forward pass produz valores inv√°lidos!\")\n",
        "                return False\n",
        "            else:\n",
        "                print(f\"‚úÖ Forward pass OK - Output range: [{test_output.min():.3f}, {test_output.max():.3f}]\")\n",
        "                return True\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Erro no forward pass: {e}\")\n",
        "            return False\n",
        "\n",
        "def create_safer_model(dropout_rate=0.2):\n",
        "    \"\"\"Cria um modelo com inicializa√ß√£o mais segura\"\"\"\n",
        "    print(\"üèóÔ∏è Criando modelo com inicializa√ß√£o segura...\")\n",
        "\n",
        "    model = AdvancedEyeTrackingCNN(dropout_rate=dropout_rate)\n",
        "\n",
        "    # Inicializa√ß√£o mais conservadora\n",
        "    for module in model.modules():\n",
        "        if isinstance(module, nn.Conv2d):\n",
        "            # Inicializa√ß√£o He/Kaiming mais conservadora\n",
        "            nn.init.kaiming_normal_(module.weight, mode='fan_out', nonlinearity='relu', a=0.1)\n",
        "            if module.bias is not None:\n",
        "                nn.init.constant_(module.bias, 0)\n",
        "        elif isinstance(module, nn.BatchNorm2d):\n",
        "            nn.init.constant_(module.weight, 1)\n",
        "            nn.init.constant_(module.bias, 0)\n",
        "        elif isinstance(module, nn.Linear):\n",
        "            # Inicializa√ß√£o Xavier mais conservadora\n",
        "            nn.init.xavier_normal_(module.weight, gain=0.5)\n",
        "            if module.bias is not None:\n",
        "                nn.init.constant_(module.bias, 0)\n",
        "\n",
        "    print(\"‚úÖ Modelo inicializado com configura√ß√£o segura\")\n",
        "    return model\n",
        "\n",
        "def debug_dataloader(dataloader, device, num_samples=3):\n",
        "    \"\"\"Debug detalhado do dataloader\"\"\"\n",
        "    print(f\"üîç Analisando {num_samples} amostras do dataloader...\")\n",
        "\n",
        "    for i, (data, target) in enumerate(dataloader):\n",
        "        if i >= num_samples:\n",
        "            break\n",
        "\n",
        "        print(f\"\\n--- Amostra {i+1} ---\")\n",
        "        print(f\"Data shape: {data.shape}\")\n",
        "        print(f\"Target shape: {target.shape}\")\n",
        "        print(f\"Data range: [{data.min():.3f}, {data.max():.3f}]\")\n",
        "        print(f\"Target range: [{target.min():.3f}, {target.max():.3f}]\")\n",
        "\n",
        "        # Verifica valores inv√°lidos\n",
        "        if torch.isnan(data).any():\n",
        "            print(\"‚ùå Data cont√©m NaN!\")\n",
        "            nan_count = torch.isnan(data).sum().item()\n",
        "            print(f\"   {nan_count} valores NaN encontrados\")\n",
        "\n",
        "        if torch.isinf(data).any():\n",
        "            print(\"‚ùå Data cont√©m Inf!\")\n",
        "            inf_count = torch.isinf(data).sum().item()\n",
        "            print(f\"   {inf_count} valores Inf encontrados\")\n",
        "\n",
        "        if torch.isnan(target).any():\n",
        "            print(\"‚ùå Target cont√©m NaN!\")\n",
        "\n",
        "        if torch.isinf(target).any():\n",
        "            print(\"‚ùå Target cont√©m Inf!\")\n",
        "\n",
        "        # Move para device e testa\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        print(f\"‚úÖ Dados movidos para {device} com sucesso\")\n",
        "\n",
        "    print(\"üîç Debug do dataloader conclu√≠do\")\n",
        "\n",
        "def fix_dataset_issues(dataset):\n",
        "    \"\"\"Tenta corrigir problemas conhecidos no dataset\"\"\"\n",
        "    print(\"üîß Verificando e corrigindo dataset...\")\n",
        "\n",
        "    # Testa algumas amostras\n",
        "    problematic_indices = []\n",
        "\n",
        "    for i in range(min(50, len(dataset))):\n",
        "        try:\n",
        "            data, target = dataset[i]\n",
        "\n",
        "            if torch.isnan(data).any() or torch.isinf(data).any():\n",
        "                problematic_indices.append(i)\n",
        "                print(f\"‚ö†Ô∏è Amostra {i}: dados problem√°ticos\")\n",
        "\n",
        "            if torch.isnan(target).any() or torch.isinf(target).any():\n",
        "                problematic_indices.append(i)\n",
        "                print(f\"‚ö†Ô∏è Amostra {i}: target problem√°tico\")\n",
        "\n",
        "        except Exception as e:\n",
        "            problematic_indices.append(i)\n",
        "            print(f\"‚ùå Erro na amostra {i}: {e}\")\n",
        "\n",
        "    if problematic_indices:\n",
        "        print(f\"‚ö†Ô∏è Encontradas {len(problematic_indices)} amostras problem√°ticas de {50} testadas\")\n",
        "        return False\n",
        "    else:\n",
        "        print(\"‚úÖ Dataset parece estar em bom estado\")\n",
        "        return True\n",
        "\n",
        "def test_system():\n",
        "    \"\"\"Testa o sistema antes de iniciar o treinamento\"\"\"\n",
        "    print(\"üß™ Testando funcionamento do sistema...\")\n",
        "\n",
        "    try:\n",
        "        # Teste PyTorch b√°sico\n",
        "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "        test_tensor = torch.randn(2, 3, 224, 224).to(device)\n",
        "        print(f\"‚úÖ Tensor de teste criado: {test_tensor.shape} em {device}\")\n",
        "\n",
        "        # Teste transforma√ß√µes b√°sicas\n",
        "        test_transform = transforms.Compose([\n",
        "            transforms.ToPILImage(),\n",
        "            transforms.Resize((224, 224)),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "        ])\n",
        "        print(\"‚úÖ Transforms funcionando\")\n",
        "\n",
        "        # Teste Albumentations\n",
        "        test_albu = A.Compose([\n",
        "            A.Resize(224, 224),\n",
        "            A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "            ToTensorV2()\n",
        "        ])\n",
        "\n",
        "        # Cria imagem de teste\n",
        "        test_image = np.random.randint(0, 255, (100, 100, 3), dtype=np.uint8)\n",
        "        transformed = test_albu(image=test_image)\n",
        "        print(\"‚úÖ Albumentations funcionando\")\n",
        "\n",
        "        # Teste OpenCV\n",
        "        test_cv_image = np.zeros((100, 100, 3), dtype=np.uint8)\n",
        "        resized = cv2.resize(test_cv_image, (224, 224))\n",
        "        print(\"‚úÖ OpenCV funcionando\")\n",
        "\n",
        "        # Teste opera√ß√£o neural b√°sica\n",
        "        test_conv = nn.Conv2d(3, 64, kernel_size=3, padding=1).to(device)\n",
        "        output = test_conv(test_tensor[:1])  # Testa s√≥ 1 imagem\n",
        "        print(f\"‚úÖ Opera√ß√£o neural: {output.shape}\")\n",
        "\n",
        "        print(\"\\nüéâ TODOS OS TESTES PASSARAM!\")\n",
        "        print(\"üöÄ Sistema pronto para treinamento!\")\n",
        "        return True\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\\n‚ùå ERRO no teste: {e}\")\n",
        "        print(\"üí° Poss√≠veis solu√ß√µes:\")\n",
        "        print(\"   1. Reinicie o runtime (Runtime ‚Üí Restart Runtime)\")\n",
        "        print(\"   2. Execute as c√©lulas de instala√ß√£o novamente\")\n",
        "        print(\"   3. Verifique se est√° usando GPU no Colab\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "        return False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d6XXPBof8fh4",
        "outputId": "f5764498-8845-4217-8532-c18574c7de6e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ 2298 labels aleat√≥rias geradas e salvas.\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import random\n",
        "\n",
        "json_path = \"/content/drive/MyDrive/GuilhermeAlmeida/smart_labels.json\"\n",
        "\n",
        "with open(json_path, \"r\") as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "# Gerar labels aleat√≥rias dentro do intervalo [0, 1]\n",
        "corrected = []\n",
        "for item in data:\n",
        "    if isinstance(item, dict) and \"image\" in item:\n",
        "        x = round(random.uniform(0.1, 0.9), 3)\n",
        "        y = round(random.uniform(0.1, 0.9), 3)\n",
        "        corrected.append({\"image\": item[\"image\"], \"label\": [x, y]})\n",
        "\n",
        "# Salvar as novas labels\n",
        "with open(json_path, \"w\") as f:\n",
        "    json.dump(corrected, f)\n",
        "\n",
        "print(f\"‚úÖ {len(corrected)} labels aleat√≥rias geradas e salvas.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "87FVUIeZ8qzy",
        "outputId": "af16b174-cdbf-4224-b5e3-59d864829c02"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ 2298 labels corrigidas com floats reais.\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "\n",
        "json_path = \"/content/drive/MyDrive/GuilhermeAlmeida/smart_labels.json\"\n",
        "\n",
        "with open(json_path, \"r\") as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "# For√ßa convers√£o para float (mesmo se parecer n√∫mero)\n",
        "corrected = []\n",
        "for item in data:\n",
        "    try:\n",
        "        x = float(item[\"label\"][0])\n",
        "        y = float(item[\"label\"][1])\n",
        "        corrected.append({\"image\": item[\"image\"], \"label\": [x, y]})\n",
        "    except:\n",
        "        continue\n",
        "\n",
        "with open(json_path, \"w\") as f:\n",
        "    json.dump(corrected, f)\n",
        "\n",
        "print(f\"‚úÖ {len(corrected)} labels corrigidas com floats reais.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PAgoe6hL8NEi",
        "outputId": "dd411908-7dee-4a81-9f1d-8008f57017db"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Labels corrigidas: 2298 v√°lidas salvas.\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "\n",
        "json_path = \"/content/drive/MyDrive/GuilhermeAlmeida/smart_labels.json\"\n",
        "\n",
        "with open(json_path, \"r\") as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "corrected = []\n",
        "if isinstance(data, dict):\n",
        "    for k, v in data.items():\n",
        "        try:\n",
        "            x = float(v[0])\n",
        "            y = float(v[1])\n",
        "            if 0 <= x <= 1 and 0 <= y <= 1:\n",
        "                corrected.append({\"image\": k, \"label\": [x, y]})\n",
        "        except:\n",
        "            continue\n",
        "\n",
        "elif isinstance(data, list):\n",
        "    for item in data:\n",
        "        try:\n",
        "            x = float(item[\"label\"][0])\n",
        "            y = float(item[\"label\"][1])\n",
        "            if 0 <= x <= 1 and 0 <= y <= 1:\n",
        "                corrected.append({\"image\": item[\"image\"], \"label\": [x, y]})\n",
        "        except:\n",
        "            continue\n",
        "\n",
        "with open(json_path, \"w\") as f:\n",
        "    json.dump(corrected, f)\n",
        "\n",
        "print(f\"‚úÖ Labels corrigidas: {len(corrected)} v√°lidas salvas.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NrBTOcPGBVdr",
        "outputId": "87274830-e210-4b66-8bfc-2279e44ca355"
      },
      "outputs": [],
      "source": [
        "# ========================================================================================\n",
        "# üöÄ FUN√á√ÉO PRINCIPAL DE TREINAMENTO SIMPLIFICADA\n",
        "# ========================================================================================\n",
        "\n",
        "def main_fixed_lr():\n",
        "    \"\"\"Fun√ß√£o principal SIMPLIFICADA para treinar com learning rate fixo\"\"\"\n",
        "    print(\"=\" * 80)\n",
        "    print(\"üîß TREINAMENTO SIMPLIFICADO - FOCO NO PROBLEMA DO LOSS\")\n",
        "    print(\"=\" * 80)\n",
        "\n",
        "    # Configura√ß√µes SIMPLIFICADAS\n",
        "    CONFIG = {\n",
        "        'BATCH_SIZE': 32,\n",
        "        'EPOCHS': 300,\n",
        "        'LEARNING_RATE': 0.0001,\n",
        "        'VALIDATION_SPLIT': 0.15,\n",
        "        'TARGET_SIZE': (224, 224),\n",
        "        'USE_AMP': True,\n",
        "        'NUM_WORKERS': 4,\n",
        "    }\n",
        "\n",
        "    print(\"üìã Configura√ß√µes SIMPLIFICADAS:\")\n",
        "    for key, value in CONFIG.items():\n",
        "        print(f\"   ‚Ä¢ {key}: {value}\")\n",
        "\n",
        "    # Device\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    print(f\"üíª Device: {device}\")\n",
        "\n",
        "    # Seeds para reprodutibilidade\n",
        "    torch.manual_seed(42)\n",
        "    np.random.seed(42)\n",
        "    random.seed(42)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed(42)\n",
        "\n",
        "    # Caminhos (ajustado para ambiente local)\n",
        "    try:\n",
        "        # Tenta primeiro os caminhos locais\n",
        "        if os.path.exists(\"captured_faces\"):\n",
        "            DATASET_PATH = \"captured_faces\"\n",
        "            LABELS_PATH = \"smart_labels.json\"\n",
        "            MODEL_SAVE_PATH = \"models/\"\n",
        "            RESULTS_PATH = \"resultados/\"\n",
        "            print(\"üìÇ Usando caminhos locais\")\n",
        "        else:\n",
        "            # Fallback para Google Drive\n",
        "            DATASET_PATH = \"/content/drive/MyDrive/GuilhermeAlmeida/fotos\"\n",
        "            LABELS_PATH = \"/content/drive/MyDrive/GuilhermeAlmeida/smart_labels.json\"\n",
        "            MODEL_SAVE_PATH = \"/content/drive/MyDrive/GuilhermeAlmeida/modelos/\"\n",
        "            RESULTS_PATH = \"/content/drive/MyDrive/GuilhermeAlmeida/resultados/\"\n",
        "            print(\"üìÇ Usando caminhos do Google Drive\")\n",
        "\n",
        "        # Cria diret√≥rios\n",
        "        os.makedirs(MODEL_SAVE_PATH, exist_ok=True)\n",
        "        os.makedirs(RESULTS_PATH, exist_ok=True)\n",
        "\n",
        "        # Verifica se existem dados\n",
        "        if not os.path.exists(DATASET_PATH):\n",
        "            print(f\"‚ùå Pasta de dados n√£o encontrada: {DATASET_PATH}\")\n",
        "            return\n",
        "\n",
        "        # Cria labels se necess√°rio\n",
        "        if not os.path.exists(LABELS_PATH):\n",
        "            print(\"üß† Criando labels inteligentes...\")\n",
        "            smart_labels_path = create_smart_labels_from_calibration()\n",
        "            if smart_labels_path is None:\n",
        "                print(\"‚ùå Falha ao criar labels. Abortando.\")\n",
        "                return\n",
        "        else:\n",
        "            smart_labels_path = LABELS_PATH\n",
        "\n",
        "        print(f\"üìÇ Dataset: {DATASET_PATH}\")\n",
        "        print(f\"üìã Labels: {smart_labels_path}\")\n",
        "\n",
        "        # Carrega dataset\n",
        "        print(\"üìÇ Carregando dataset...\")\n",
        "        full_dataset = AdvancedEyeTrackingDataset(\n",
        "            images_path=DATASET_PATH,\n",
        "            labels_path=smart_labels_path,\n",
        "            augment=False,  # SEM AUGMENTA√á√ÉO para debugging\n",
        "            target_size=CONFIG['TARGET_SIZE']\n",
        "        )\n",
        "\n",
        "        if len(full_dataset) == 0:\n",
        "            print(\"‚ùå Dataset vazio!\")\n",
        "            return\n",
        "\n",
        "        print(f\"‚úÖ Dataset carregado: {len(full_dataset)} amostras\")\n",
        "\n",
        "        # Teste b√°sico do dataset\n",
        "        try:\n",
        "            sample_data, sample_label = full_dataset[0]\n",
        "            # Verifica se h√° valores inv√°lidos\n",
        "            if torch.isnan(sample_data).any() or torch.isnan(sample_label).any():\n",
        "                print(\"‚ùå Dataset cont√©m valores inv√°lidos!\")\n",
        "                return\n",
        "            print(\"‚úÖ Dataset validado\")\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Erro ao validar dataset: {e}\")\n",
        "            return\n",
        "\n",
        "        # Divide dataset\n",
        "        train_size = int((1 - CONFIG['VALIDATION_SPLIT']) * len(full_dataset))\n",
        "        val_size = len(full_dataset) - train_size\n",
        "\n",
        "        train_dataset, val_dataset = torch.utils.data.random_split(\n",
        "            full_dataset, [train_size, val_size],\n",
        "            generator=torch.Generator().manual_seed(42)\n",
        "        )\n",
        "\n",
        "        # Data loaders SIMPLES\n",
        "        train_loader = DataLoader(\n",
        "            train_dataset,\n",
        "            batch_size=CONFIG['BATCH_SIZE'],\n",
        "            shuffle=True,\n",
        "            num_workers=0,\n",
        "            pin_memory=False,\n",
        "            drop_last=True\n",
        "        )\n",
        "\n",
        "        val_loader = DataLoader(\n",
        "            val_dataset,\n",
        "            batch_size=CONFIG['BATCH_SIZE'],\n",
        "            shuffle=False,\n",
        "            num_workers=0,\n",
        "            pin_memory=False\n",
        "        )\n",
        "\n",
        "        print(f\"üîÑ Divis√£o - Treino: {len(train_dataset)} | Valida√ß√£o: {len(val_dataset)}\")\n",
        "\n",
        "        # TESTE SIMPLIFICADO DO DATALOADER\n",
        "        try:\n",
        "            test_batch = next(iter(train_loader))\n",
        "            data, target = test_batch\n",
        "            # Verifica valores inv√°lidos\n",
        "            if torch.isnan(data).any() or torch.isnan(target).any():\n",
        "                print(\"‚ùå Dataloader cont√©m valores inv√°lidos!\")\n",
        "                return\n",
        "            print(\"‚úÖ Dataloader validado\")\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Erro no dataloader: {e}\")\n",
        "            return\n",
        "\n",
        "        # Cria modelo SIMPLES\n",
        "        print(\"üèóÔ∏è Criando modelo...\")\n",
        "        model = AdvancedEyeTrackingCNN(dropout_rate=0.1)  # Dropout baixo\n",
        "\n",
        "        # Inicializa√ß√£o mais conservadora\n",
        "        for m in model.modules():\n",
        "            if isinstance(m, nn.Linear):\n",
        "                nn.init.xavier_normal_(m.weight, gain=0.1)  # Gain muito baixo\n",
        "                if m.bias is not None:\n",
        "                    nn.init.constant_(m.bias, 0)\n",
        "\n",
        "        model.to(device)\n",
        "        total_params = sum(p.numel() for p in model.parameters())\n",
        "        print(f\"üß† Modelo: {total_params:,} par√¢metros\")\n",
        "\n",
        "        # TESTE SIMPLIFICADO DO MODELO\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            test_batch = next(iter(train_loader))\n",
        "            test_data, test_target = test_batch\n",
        "            test_data, test_target = test_data.to(device), test_target.to(device)\n",
        "\n",
        "            test_output = model(test_data)\n",
        "            mse_loss = F.mse_loss(test_output, test_target)\n",
        "\n",
        "            if torch.isnan(test_output).any() or torch.isnan(mse_loss):\n",
        "                print(\"‚ùå Modelo produz valores inv√°lidos!\")\n",
        "                return\n",
        "\n",
        "            print(\"‚úÖ Modelo validado\")\n",
        "\n",
        "        # Cria trainer SIMPLIFICADO\n",
        "        trainer = FixedLRAdvancedEyeTrackingTrainer(\n",
        "            model, device,\n",
        "            use_amp=False,\n",
        "            save_path=MODEL_SAVE_PATH,\n",
        "            results_path=RESULTS_PATH\n",
        "        )\n",
        "\n",
        "        # INICIA TREINAMENTO\n",
        "        print(\"\\nüöÄ Iniciando treinamento...\")\n",
        "        start_time = time.time()\n",
        "\n",
        "        train_losses, val_losses, train_accs, val_accs = trainer.train(\n",
        "            train_loader, val_loader,\n",
        "            epochs=CONFIG['EPOCHS'],\n",
        "            learning_rate=CONFIG['LEARNING_RATE'],\n",
        "            use_fixed_lr=True\n",
        "        )\n",
        "\n",
        "        training_time = time.time() - start_time\n",
        "        print(f\"\\n‚è±Ô∏è  Tempo total: {training_time:.2f} segundos\")\n",
        "\n",
        "        print(\"\\nüéâ TREINAMENTO CONCLU√çDO!\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Erro cr√≠tico: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "\n",
        "# ========================================================================================\n",
        "# üöÄ EXECU√á√ÉO PRINCIPAL SIMPLIFICADA\n",
        "# ========================================================================================\n",
        "\n",
        "print(\"üî• Iniciando sistema SIMPLIFICADO de treinamento...\")\n",
        "\n",
        "# Executa teste do sistema\n",
        "if test_system():\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"üöÄ EXECUTANDO TREINAMENTO SIMPLIFICADO\")\n",
        "    print(\"=\"*80)\n",
        "    main_fixed_lr()\n",
        "else:\n",
        "    print(\"\\n‚ùå Sistema n√£o passou nos testes!\")\n",
        "    print(\"üîß Corrija os erros antes de continuar.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "846idU1CpOTN",
        "outputId": "aab7bc71-e0c5-467e-9f30-77ed369bf8a3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Labels convertidas corretamente: 2298 entradas salvas.\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "\n",
        "# Caminho do JSON\n",
        "json_path = \"/content/drive/MyDrive/GuilhermeAlmeida/smart_labels.json\"\n",
        "\n",
        "# Corrige o formato\n",
        "with open(json_path, \"r\") as f:\n",
        "    raw_labels = json.load(f)\n",
        "\n",
        "# Converte o dicion√°rio para uma lista de dicion√°rios\n",
        "labels_list = [{\"image\": k, \"label\": v} for k, v in raw_labels.items()]\n",
        "\n",
        "# Salva de volta no formato corrigido\n",
        "with open(json_path, \"w\") as f:\n",
        "    json.dump(labels_list, f)\n",
        "\n",
        "print(f\"‚úÖ Labels convertidas corretamente: {len(labels_list)} entradas salvas.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "pVE_53s7t3pK"
      },
      "outputs": [],
      "source": [
        "# ========================================================================================\n",
        "# üìä CLASSE PARA MANIPULAR M√âTRICAS E GR√ÅFICOS\n",
        "# ========================================================================================\n",
        "\n",
        "class MetricsHandler:\n",
        "    \"\"\"Classe auxiliar para manipular m√©tricas e gr√°ficos\"\"\"\n",
        "\n",
        "    def __init__(self, results_path='resultados/'):\n",
        "        self.results_path = results_path\n",
        "        os.makedirs(os.path.join(self.results_path, 'metricas'), exist_ok=True)\n",
        "\n",
        "    def save_advanced_metrics(self, metrics_data, filename):\n",
        "        \"\"\"Salva m√©tricas avan√ßadas\"\"\"\n",
        "        metrics_path = os.path.join(self.results_path, 'metricas', filename)\n",
        "        os.makedirs(os.path.dirname(metrics_path), exist_ok=True)\n",
        "\n",
        "        with open(metrics_path, 'w') as f:\n",
        "            json.dump(metrics_data, f, indent=2)\n",
        "\n",
        "        print(f\"üìä M√©tricas salvas: {metrics_path}\")\n",
        "\n",
        "    def plot_training_curves(self, train_losses, val_losses, train_accs, val_accs, learning_rates, save_path=None):\n",
        "        \"\"\"Plota curvas de treinamento avan√ßadas\"\"\"\n",
        "        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(16, 12))\n",
        "\n",
        "        epochs = range(1, len(train_losses) + 1)\n",
        "\n",
        "        # Plot das perdas\n",
        "        ax1.plot(epochs, train_losses, 'b-', label='Perda Treino', linewidth=2)\n",
        "        ax1.plot(epochs, val_losses, 'r-', label='Perda Valida√ß√£o', linewidth=2)\n",
        "        ax1.set_title('Curvas de Perda', fontsize=14, fontweight='bold')\n",
        "        ax1.set_xlabel('√âpoca')\n",
        "        ax1.set_ylabel('Perda Combinada')\n",
        "        ax1.legend()\n",
        "        ax1.grid(True, alpha=0.3)\n",
        "\n",
        "        # Plot das acur√°cias (erro euclidiano)\n",
        "        ax2.plot(epochs, train_accs, 'b-', label='Erro Treino', linewidth=2)\n",
        "        ax2.plot(epochs, val_accs, 'r-', label='Erro Valida√ß√£o', linewidth=2)\n",
        "        ax2.set_title('Erro Euclidiano', fontsize=14, fontweight='bold')\n",
        "        ax2.set_xlabel('√âpoca')\n",
        "        ax2.set_ylabel('Dist√¢ncia Euclidiana')\n",
        "        ax2.legend()\n",
        "        ax2.grid(True, alpha=0.3)\n",
        "\n",
        "        # Learning rate\n",
        "        if learning_rates:\n",
        "            ax3.plot(epochs, learning_rates, 'g-', linewidth=2)\n",
        "            ax3.set_title('Learning Rate', fontsize=14, fontweight='bold')\n",
        "            ax3.set_xlabel('√âpoca')\n",
        "            ax3.set_ylabel('Learning Rate')\n",
        "            ax3.set_yscale('log')\n",
        "            ax3.grid(True, alpha=0.3)\n",
        "\n",
        "        # Overfitting detection\n",
        "        if len(train_losses) > 10:\n",
        "            train_smooth = np.convolve(train_losses, np.ones(5) / 5, mode='valid')\n",
        "            val_smooth = np.convolve(val_losses, np.ones(5) / 5, mode='valid')\n",
        "            gap = val_smooth - train_smooth\n",
        "            ax4.plot(range(3, len(gap) + 3), gap, 'purple', linewidth=2)\n",
        "            ax4.axhline(y=0, color='black', linestyle='--', alpha=0.5)\n",
        "            ax4.set_title('Gap Treino-Valida√ß√£o (Overfitting)', fontsize=14, fontweight='bold')\n",
        "            ax4.set_xlabel('√âpoca')\n",
        "            ax4.set_ylabel('Val Loss - Train Loss')\n",
        "            ax4.grid(True, alpha=0.3)\n",
        "\n",
        "        plt.tight_layout()\n",
        "\n",
        "        if save_path:\n",
        "            # Salva no caminho especificado\n",
        "            full_save_path = os.path.join(self.results_path, save_path)\n",
        "            os.makedirs(os.path.dirname(full_save_path), exist_ok=True)\n",
        "            plt.savefig(full_save_path, dpi=300, bbox_inches='tight')\n",
        "            print(f\"üìà Gr√°fico salvo: {full_save_path}\")\n",
        "\n",
        "        plt.show()\n",
        "        plt.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "JxKXU2G7t3pL"
      },
      "outputs": [],
      "source": [
        "# ========================================================================================\n",
        "# üß† FUN√á√ÉO PARA CRIAR LABELS INTELIGENTES\n",
        "# ========================================================================================\n",
        "\n",
        "def create_smart_labels_from_calibration():\n",
        "    \"\"\"Cria labels inteligentes baseados em padr√µes de captura\"\"\"\n",
        "\n",
        "    # Tenta primeiro caminhos locais\n",
        "    if os.path.exists(\"captured_faces\"):\n",
        "        captured_faces_path = \"captured_faces\"\n",
        "        labels_path = \"smart_labels.json\"\n",
        "        print(\"üìÇ Usando caminhos locais para labels\")\n",
        "    else:\n",
        "        # Fallback para Google Drive\n",
        "        captured_faces_path = \"/content/drive/MyDrive/GuilhermeAlmeida/fotos\"\n",
        "        labels_path = \"/content/drive/MyDrive/GuilhermeAlmeida/smart_labels.json\"\n",
        "        print(\"üìÇ Usando caminhos do Google Drive para labels\")\n",
        "\n",
        "    if not os.path.exists(captured_faces_path):\n",
        "        print(f\"‚ùå Pasta {captured_faces_path} n√£o encontrada.\")\n",
        "        return None\n",
        "\n",
        "    # Coleta todas as imagens\n",
        "    image_files = []\n",
        "    for file in sorted(os.listdir(captured_faces_path)):\n",
        "        if file.lower().endswith(('.jpg', '.jpeg', '.png')):\n",
        "            image_files.append(file)\n",
        "\n",
        "    if not image_files:\n",
        "        print(\"‚ùå Nenhuma imagem encontrada na pasta.\")\n",
        "        return None\n",
        "\n",
        "    print(f\"üì∏ Encontradas {len(image_files)} imagens para labeling\")\n",
        "\n",
        "    # Pontos de calibra√ß√£o t√≠picos em grid 3x3\n",
        "    calibration_points = [\n",
        "        (0.1, 0.1), (0.5, 0.1), (0.9, 0.1),  # Linha superior\n",
        "        (0.1, 0.5), (0.5, 0.5), (0.9, 0.5),  # Linha central\n",
        "        (0.1, 0.9), (0.5, 0.9), (0.9, 0.9)   # Linha inferior\n",
        "    ]\n",
        "\n",
        "    # Cria labels para todas as imagens\n",
        "    labels = []\n",
        "    for i, img_file in enumerate(image_files):\n",
        "        # Usa padr√£o circular atrav√©s dos pontos de calibra√ß√£o\n",
        "        point_idx = i % len(calibration_points)\n",
        "        gaze_x, gaze_y = calibration_points[point_idx]\n",
        "\n",
        "        # Adiciona varia√ß√£o pequena para simular movimento natural\n",
        "        gaze_x += np.random.normal(0, 0.03)\n",
        "        gaze_y += np.random.normal(0, 0.03)\n",
        "\n",
        "        # Garante que est√° no range [0, 1]\n",
        "        gaze_x = np.clip(gaze_x, 0.05, 0.95)\n",
        "        gaze_y = np.clip(gaze_y, 0.05, 0.95)\n",
        "\n",
        "        labels.append([float(gaze_x), float(gaze_y)])\n",
        "\n",
        "    # Salva dados organizados\n",
        "    dataset_info = {\n",
        "        'images': image_files,\n",
        "        'labels': labels,\n",
        "        'num_samples': len(labels),\n",
        "        'created_at': time.time(),\n",
        "        'images_path': captured_faces_path\n",
        "    }\n",
        "\n",
        "    # Cria diret√≥rio se n√£o existir\n",
        "    os.makedirs(os.path.dirname(labels_path), exist_ok=True)\n",
        "\n",
        "    with open(labels_path, 'w') as f:\n",
        "        json.dump(dataset_info, f, indent=2)\n",
        "\n",
        "    print(f\"‚úÖ Labels inteligentes criados para {len(labels)} imagens\")\n",
        "    print(f\"üìÇ Salvo em: {labels_path}\")\n",
        "\n",
        "    return labels_path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yQzRePn72een",
        "outputId": "5bf3fbbd-2dbc-4007-fe16-65776a70a99d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üìÇ Labels antigas carregadas: 2298 imagens rotuladas.\n",
            "‚úÖ Labels atualizadas com sucesso!\n",
            "Total de imagens rotuladas agora: 2298\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import json\n",
        "\n",
        "# Caminhos\n",
        "image_folder = \"/content/drive/MyDrive/GuilhermeAlmeida/fotos\"\n",
        "json_path = \"/content/drive/MyDrive/GuilhermeAlmeida/smart_labels.json\"\n",
        "\n",
        "# Carrega as labels antigas, se existirem\n",
        "if os.path.exists(json_path):\n",
        "    with open(json_path, 'r') as f:\n",
        "        old_labels = json.load(f)\n",
        "    print(f\"üìÇ Labels antigas carregadas: {len(old_labels)} imagens rotuladas.\")\n",
        "else:\n",
        "    old_labels = {}\n",
        "    print(\"‚ö† Nenhuma label antiga encontrada, criando do zero.\")\n",
        "\n",
        "# Lista todas as imagens atuais\n",
        "valid_extensions = (\".jpg\", \".jpeg\", \".png\")\n",
        "image_files = sorted([f for f in os.listdir(image_folder) if f.lower().endswith(valid_extensions)])\n",
        "\n",
        "# Atualiza ou adiciona labels para todas as imagens\n",
        "updated_labels = {}\n",
        "for img in image_files:\n",
        "    if img in old_labels:\n",
        "        updated_labels[img] = old_labels[img]\n",
        "    else:\n",
        "        updated_labels[img] = \"desconhecido\"  # Label padr√£o para novas imagens\n",
        "\n",
        "# Salva o JSON atualizado\n",
        "with open(json_path, 'w') as f:\n",
        "    json.dump(updated_labels, f, indent=4)\n",
        "\n",
        "print(f\"‚úÖ Labels atualizadas com sucesso!\")\n",
        "print(f\"Total de imagens rotuladas agora: {len(updated_labels)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KNonm_ZNvxU4",
        "outputId": "537ece85-a3f7-49fd-863f-c0c9fc870adc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Labels convertidas corretamente: 2298 entradas salvas.\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "\n",
        "# Caminho do JSON\n",
        "json_path = \"/content/drive/MyDrive/GuilhermeAlmeida/smart_labels.json\"\n",
        "\n",
        "# Corrige o formato\n",
        "with open(json_path, \"r\") as f:\n",
        "    raw_labels = json.load(f)\n",
        "\n",
        "# Converte o dicion√°rio para uma lista de dicion√°rios\n",
        "labels_list = [{\"image\": k, \"label\": v} for k, v in raw_labels.items()]\n",
        "\n",
        "# Salva de volta no formato corrigido\n",
        "with open(json_path, \"w\") as f:\n",
        "    json.dump(labels_list, f)\n",
        "\n",
        "print(f\"‚úÖ Labels convertidas corretamente: {len(labels_list)} entradas salvas.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EYvWK_yPv69X"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ========================================================================================\n",
        "# üöÄ C√ìDIGO COMPLETO CORRIGIDO COM LEARNING RATE SCHEDULER AVAN√áADO\n",
        "# ========================================================================================\n",
        "\n",
        "# ========================================================================================\n",
        "# üìä TRAINER AVAN√áADO COM M√öLTIPLOS SCHEDULERS\n",
        "# ========================================================================================\n",
        "\n",
        "class AdvancedEyeTrackingTrainerWithScheduler:\n",
        "    \"\"\"Trainer avan√ßado com learning rate scheduler e otimiza√ß√µes completas\"\"\"\n",
        "    \n",
        "    def __init__(self, model, device, use_amp=True, save_path=\"./modelo.pth\", results_path=\"./resultados\"):\n",
        "        self.model = model.to(device)\n",
        "        self.device = device\n",
        "        self.use_amp = use_amp\n",
        "        self.save_path = save_path\n",
        "        self.results_path = results_path\n",
        "        \n",
        "        # M√©tricas de treinamento\n",
        "        self.train_losses = []\n",
        "        self.val_losses = []\n",
        "        self.train_accuracies = []\n",
        "        self.val_accuracies = []\n",
        "        self.learning_rates = []  # Para tracking do LR\n",
        "        \n",
        "        # Melhores m√©tricas\n",
        "        self.best_metrics = {\n",
        "            \"epoch\": 0,\n",
        "            \"val_loss\": float('inf'),\n",
        "            \"best_acc_20\": 0.0\n",
        "        }\n",
        "        \n",
        "        # AMP Scaler se habilitado\n",
        "        if self.use_amp:\n",
        "            self.scaler = torch.cuda.amp.GradScaler()\n",
        "            print(\"‚úÖ AMP (Automatic Mixed Precision) habilitado\")\n",
        "        \n",
        "        print(f\"üß† Trainer configurado para device: {device}\")\n",
        "    \n",
        "    def _create_optimizers_and_schedulers(self, learning_rate, scheduler_type=\"cosine_with_warmup\"):\n",
        "        \"\"\"Cria otimizador e scheduler baseado no tipo escolhido\"\"\"\n",
        "        \n",
        "        # Otimizador AdamW com weight decay\n",
        "        self.optimizer = optim.AdamW(\n",
        "            self.model.parameters(),\n",
        "            lr=learning_rate,\n",
        "            weight_decay=1e-4,\n",
        "            betas=(0.9, 0.999),\n",
        "            eps=1e-8\n",
        "        )\n",
        "        \n",
        "        # Configura√ß√£o de schedulers diferentes\n",
        "        if scheduler_type == \"cosine_with_warmup\":\n",
        "            # Cosine Annealing com Warm-up (mais moderno)\n",
        "            warmup_epochs = 10\n",
        "            total_epochs = 300\n",
        "            \n",
        "            def lr_lambda(epoch):\n",
        "                if epoch < warmup_epochs:\n",
        "                    # Warm-up linear\n",
        "                    return epoch / warmup_epochs\n",
        "                else:\n",
        "                    # Cosine annealing\n",
        "                    progress = (epoch - warmup_epochs) / (total_epochs - warmup_epochs)\n",
        "                    return 0.5 * (1 + np.cos(np.pi * progress))\n",
        "            \n",
        "            self.scheduler = optim.lr_scheduler.LambdaLR(self.optimizer, lr_lambda)\n",
        "            print(\"üìà Scheduler: Cosine Annealing com Warm-up\")\n",
        "            \n",
        "        elif scheduler_type == \"reduce_on_plateau\":\n",
        "            # Reduz LR quando loss para de melhorar\n",
        "            self.scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
        "                self.optimizer, \n",
        "                mode='min', \n",
        "                factor=0.5, \n",
        "                patience=15, \n",
        "                verbose=True,\n",
        "                min_lr=1e-7\n",
        "            )\n",
        "            print(\"üìâ Scheduler: Reduce on Plateau\")\n",
        "            \n",
        "        elif scheduler_type == \"step_lr\":\n",
        "            # Step LR cl√°ssico\n",
        "            self.scheduler = optim.lr_scheduler.StepLR(\n",
        "                self.optimizer, \n",
        "                step_size=50, \n",
        "                gamma=0.3\n",
        "            )\n",
        "            print(\"üìä Scheduler: Step LR\")\n",
        "            \n",
        "        elif scheduler_type == \"exponential\":\n",
        "            # Decay exponencial\n",
        "            self.scheduler = optim.lr_scheduler.ExponentialLR(\n",
        "                self.optimizer, \n",
        "                gamma=0.99\n",
        "            )\n",
        "            print(\"üìà Scheduler: Exponential Decay\")\n",
        "            \n",
        "        elif scheduler_type == \"cyclic\":\n",
        "            # Cyclic LR para escape de m√≠nimos locais\n",
        "            self.scheduler = optim.lr_scheduler.CyclicLR(\n",
        "                self.optimizer,\n",
        "                base_lr=learning_rate * 0.1,\n",
        "                max_lr=learning_rate,\n",
        "                step_size_up=30,\n",
        "                mode='triangular2'\n",
        "            )\n",
        "            print(\"üîÑ Scheduler: Cyclic LR\")\n",
        "            \n",
        "        else:\n",
        "            # Sem scheduler\n",
        "            self.scheduler = None\n",
        "            print(\"‚û°Ô∏è Scheduler: Nenhum (LR fixo)\")\n",
        "        \n",
        "        self.scheduler_type = scheduler_type\n",
        "    \n",
        "    def calculate_metrics(self, outputs, targets, threshold_20=0.2):\n",
        "        \"\"\"Calcula m√©tricas de precis√£o para eye tracking\"\"\"\n",
        "        with torch.no_grad():\n",
        "            # Dist√¢ncia euclidiana normalizada\n",
        "            distances = torch.sqrt(torch.sum((outputs - targets) ** 2, dim=1))\n",
        "            \n",
        "            # Accuracy dentro de threshold (20% da tela)\n",
        "            accuracy_20 = (distances <= threshold_20).float().mean().item()\n",
        "            \n",
        "            # Dist√¢ncia m√©dia\n",
        "            mean_distance = distances.mean().item()\n",
        "            \n",
        "            return {\n",
        "                'accuracy_20': accuracy_20,\n",
        "                'mean_distance': mean_distance,\n",
        "                'distances': distances\n",
        "            }\n",
        "    \n",
        "    def train_epoch(self, train_loader):\n",
        "        \"\"\"Treina uma √©poca\"\"\"\n",
        "        self.model.train()\n",
        "        epoch_loss = 0.0\n",
        "        epoch_acc_20 = 0.0\n",
        "        num_batches = 0\n",
        "        \n",
        "        progress_bar = tqdm(train_loader, desc=\"Treinando\", leave=False)\n",
        "        \n",
        "        for batch_idx, (data, target) in enumerate(progress_bar):\n",
        "            data, target = data.to(self.device), target.to(self.device)\n",
        "            \n",
        "            self.optimizer.zero_grad()\n",
        "            \n",
        "            if self.use_amp:\n",
        "                # Forward pass com AMP\n",
        "                with torch.cuda.amp.autocast():\n",
        "                    output = self.model(data)\n",
        "                    loss = F.mse_loss(output, target)\n",
        "                \n",
        "                # Backward pass com AMP\n",
        "                self.scaler.scale(loss).backward()\n",
        "                \n",
        "                # Gradient clipping\n",
        "                self.scaler.unscale_(self.optimizer)\n",
        "                torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)\n",
        "                \n",
        "                self.scaler.step(self.optimizer)\n",
        "                self.scaler.update()\n",
        "            else:\n",
        "                # Forward pass normal\n",
        "                output = self.model(data)\n",
        "                loss = F.mse_loss(output, target)\n",
        "                \n",
        "                # Backward pass normal\n",
        "                loss.backward()\n",
        "                \n",
        "                # Gradient clipping\n",
        "                torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)\n",
        "                \n",
        "                self.optimizer.step()\n",
        "            \n",
        "            # Atualiza scheduler se for cyclic (a cada batch)\n",
        "            if self.scheduler_type == \"cyclic\" and self.scheduler is not None:\n",
        "                self.scheduler.step()\n",
        "            \n",
        "            # Calcula m√©tricas\n",
        "            metrics = self.calculate_metrics(output, target)\n",
        "            \n",
        "            epoch_loss += loss.item()\n",
        "            epoch_acc_20 += metrics['accuracy_20']\n",
        "            num_batches += 1\n",
        "            \n",
        "            # Atualiza progress bar\n",
        "            current_lr = self.optimizer.param_groups[0]['lr']\n",
        "            progress_bar.set_postfix({\n",
        "                'Loss': f'{loss.item():.4f}',\n",
        "                'Acc@20%': f'{metrics[\"accuracy_20\"]:.3f}',\n",
        "                'LR': f'{current_lr:.2e}'\n",
        "            })\n",
        "            \n",
        "            # Log detalhado a cada 50 batches\n",
        "            if batch_idx % 50 == 0:\n",
        "                print(f\"   Batch {batch_idx}: Loss={loss.item():.4f}, Acc@20%={metrics['accuracy_20']:.3f}, LR={current_lr:.2e}\")\n",
        "        \n",
        "        return epoch_loss / num_batches, epoch_acc_20 / num_batches\n",
        "    \n",
        "    def validate_epoch(self, val_loader):\n",
        "        \"\"\"Valida uma √©poca\"\"\"\n",
        "        self.model.eval()\n",
        "        epoch_loss = 0.0\n",
        "        epoch_acc_20 = 0.0\n",
        "        num_batches = 0\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            for data, target in tqdm(val_loader, desc=\"Validando\", leave=False):\n",
        "                data, target = data.to(self.device), target.to(self.device)\n",
        "                \n",
        "                if self.use_amp:\n",
        "                    with torch.cuda.amp.autocast():\n",
        "                        output = self.model(data)\n",
        "                        loss = F.mse_loss(output, target)\n",
        "                else:\n",
        "                    output = self.model(data)\n",
        "                    loss = F.mse_loss(output, target)\n",
        "                \n",
        "                metrics = self.calculate_metrics(output, target)\n",
        "                \n",
        "                epoch_loss += loss.item()\n",
        "                epoch_acc_20 += metrics['accuracy_20']\n",
        "                num_batches += 1\n",
        "        \n",
        "        return epoch_loss / num_batches, epoch_acc_20 / num_batches\n",
        "    \n",
        "    def train(self, train_loader, val_loader, epochs=300, learning_rate=0.0001, \n",
        "              scheduler_type=\"cosine_with_warmup\", patience=30):\n",
        "        \"\"\"\n",
        "        Treina o modelo com scheduler avan√ßado\n",
        "        \n",
        "        scheduler_type op√ß√µes:\n",
        "        - \"cosine_with_warmup\": Cosine annealing com warm-up (RECOMENDADO)\n",
        "        - \"reduce_on_plateau\": Reduz LR quando loss para de melhorar\n",
        "        - \"step_lr\": Step LR cl√°ssico\n",
        "        - \"exponential\": Decay exponencial\n",
        "        - \"cyclic\": Cyclic LR\n",
        "        - \"none\": Sem scheduler\n",
        "        \"\"\"\n",
        "        \n",
        "        print(f\"\\nüöÄ INICIANDO TREINAMENTO AVAN√áADO\")\n",
        "        print(f\"üìã Configura√ß√µes:\")\n",
        "        print(f\"   ‚Ä¢ √âpocas: {epochs}\")\n",
        "        print(f\"   ‚Ä¢ Learning Rate inicial: {learning_rate}\")\n",
        "        print(f\"   ‚Ä¢ Scheduler: {scheduler_type}\")\n",
        "        print(f\"   ‚Ä¢ Patience: {patience}\")\n",
        "        print(f\"   ‚Ä¢ AMP: {self.use_amp}\")\n",
        "        print(f\"   ‚Ä¢ Device: {self.device}\")\n",
        "        \n",
        "        # Cria otimizador e scheduler\n",
        "        self._create_optimizers_and_schedulers(learning_rate, scheduler_type)\n",
        "        \n",
        "        # Vari√°veis de controle\n",
        "        patience_counter = 0\n",
        "        start_time = time.time()\n",
        "        \n",
        "        for epoch in range(epochs):\n",
        "            epoch_start = time.time()\n",
        "            \n",
        "            print(f\"\\nüìÖ √âpoca {epoch+1}/{epochs}\")\n",
        "            current_lr = self.optimizer.param_groups[0]['lr']\n",
        "            print(f\"üìä Learning Rate atual: {current_lr:.2e}\")\n",
        "            \n",
        "            # Treinamento\n",
        "            train_loss, train_acc = self.train_epoch(train_loader)\n",
        "            \n",
        "            # Valida√ß√£o\n",
        "            val_loss, val_acc = self.validate_epoch(val_loader)\n",
        "            \n",
        "            # Atualiza scheduler (exceto cyclic que √© por batch)\n",
        "            if self.scheduler is not None and self.scheduler_type != \"cyclic\":\n",
        "                if self.scheduler_type == \"reduce_on_plateau\":\n",
        "                    self.scheduler.step(val_loss)\n",
        "                else:\n",
        "                    self.scheduler.step()\n",
        "            \n",
        "            # Salva m√©tricas\n",
        "            self.train_losses.append(train_loss)\n",
        "            self.val_losses.append(val_loss)\n",
        "            self.train_accuracies.append(train_acc)\n",
        "            self.val_accuracies.append(val_acc)\n",
        "            self.learning_rates.append(current_lr)\n",
        "            \n",
        "            epoch_time = time.time() - epoch_start\n",
        "            \n",
        "            # Log da √©poca\n",
        "            print(f\"üìä Resultados da √âpoca {epoch+1}:\")\n",
        "            print(f\"   ‚Ä¢ Train Loss: {train_loss:.4f} | Train Acc@20%: {train_acc:.3f}\")\n",
        "            print(f\"   ‚Ä¢ Val Loss: {val_loss:.4f} | Val Acc@20%: {val_acc:.3f}\")\n",
        "            print(f\"   ‚Ä¢ Learning Rate: {current_lr:.2e}\")\n",
        "            print(f\"   ‚Ä¢ Tempo: {epoch_time:.2f}s\")\n",
        "            \n",
        "            # Verifica se √© o melhor modelo\n",
        "            if val_acc > self.best_metrics[\"best_acc_20\"]:\n",
        "                self.best_metrics.update({\n",
        "                    \"epoch\": epoch + 1,\n",
        "                    \"val_loss\": val_loss,\n",
        "                    \"best_acc_20\": val_acc\n",
        "                })\n",
        "                \n",
        "                # Salva melhor modelo\n",
        "                torch.save({\n",
        "                    'epoch': epoch + 1,\n",
        "                    'model_state_dict': self.model.state_dict(),\n",
        "                    'optimizer_state_dict': self.optimizer.state_dict(),\n",
        "                    'scheduler_state_dict': self.scheduler.state_dict() if self.scheduler else None,\n",
        "                    'train_loss': train_loss,\n",
        "                    'val_loss': val_loss,\n",
        "                    'val_acc': val_acc,\n",
        "                    'best_metrics': self.best_metrics\n",
        "                }, self.save_path)\n",
        "                \n",
        "                print(f\"‚úÖ NOVO MELHOR MODELO! Acc@20%: {val_acc:.3f}\")\n",
        "                patience_counter = 0\n",
        "            else:\n",
        "                patience_counter += 1\n",
        "            \n",
        "            # Early stopping\n",
        "            if patience_counter >= patience:\n",
        "                print(f\"\\n‚èπÔ∏è Early stopping acionado ap√≥s {patience} √©pocas sem melhoria\")\n",
        "                break\n",
        "            \n",
        "            # Salva gr√°ficos periodicamente\n",
        "            if (epoch + 1) % 20 == 0:\n",
        "                self.plot_training_curves()\n",
        "        \n",
        "        training_time = time.time() - start_time\n",
        "        \n",
        "        print(f\"\\nüéâ TREINAMENTO CONCLU√çDO!\")\n",
        "        print(f\"‚è±Ô∏è Tempo total: {training_time/3600:.2f} horas\")\n",
        "        print(f\"üèÜ Melhor √©poca: {self.best_metrics['epoch']}\")\n",
        "        print(f\"üéØ Melhor Acc@20%: {self.best_metrics['best_acc_20']:.3f}\")\n",
        "        \n",
        "        # Gr√°ficos finais\n",
        "        self.plot_training_curves()\n",
        "        self.plot_learning_rate_curve()\n",
        "        \n",
        "        return self.train_losses, self.val_losses, self.train_accuracies, self.val_accuracies\n",
        "    \n",
        "    def plot_training_curves(self):\n",
        "        \"\"\"Plota curvas de treinamento\"\"\"\n",
        "        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))\n",
        "        \n",
        "        # Loss curves\n",
        "        ax1.plot(self.train_losses, label='Train Loss', color='blue')\n",
        "        ax1.plot(self.val_losses, label='Val Loss', color='red')\n",
        "        ax1.set_title('Training and Validation Loss')\n",
        "        ax1.set_xlabel('Epoch')\n",
        "        ax1.set_ylabel('Loss')\n",
        "        ax1.legend()\n",
        "        ax1.grid(True)\n",
        "        \n",
        "        # Accuracy curves\n",
        "        ax2.plot(self.train_accuracies, label='Train Acc@20%', color='blue')\n",
        "        ax2.plot(self.val_accuracies, label='Val Acc@20%', color='red')\n",
        "        ax2.set_title('Training and Validation Accuracy')\n",
        "        ax2.set_xlabel('Epoch')\n",
        "        ax2.set_ylabel('Accuracy@20%')\n",
        "        ax2.legend()\n",
        "        ax2.grid(True)\n",
        "        \n",
        "        # Learning Rate\n",
        "        ax3.plot(self.learning_rates, color='green')\n",
        "        ax3.set_title('Learning Rate Schedule')\n",
        "        ax3.set_xlabel('Epoch')\n",
        "        ax3.set_ylabel('Learning Rate')\n",
        "        ax3.set_yscale('log')\n",
        "        ax3.grid(True)\n",
        "        \n",
        "        # Loss zoom (√∫ltimas 50 √©pocas)\n",
        "        if len(self.val_losses) > 50:\n",
        "            ax4.plot(self.train_losses[-50:], label='Train Loss', color='blue')\n",
        "            ax4.plot(self.val_losses[-50:], label='Val Loss', color='red')\n",
        "            ax4.set_title('Loss (Last 50 Epochs)')\n",
        "        else:\n",
        "            ax4.plot(self.train_losses, label='Train Loss', color='blue')\n",
        "            ax4.plot(self.val_losses, label='Val Loss', color='red')\n",
        "            ax4.set_title('Loss (All Epochs)')\n",
        "        ax4.set_xlabel('Epoch')\n",
        "        ax4.set_ylabel('Loss')\n",
        "        ax4.legend()\n",
        "        ax4.grid(True)\n",
        "        \n",
        "        plt.tight_layout()\n",
        "        plt.savefig(f'{self.results_path}/training_curves.png', dpi=300, bbox_inches='tight')\n",
        "        plt.show()\n",
        "    \n",
        "    def plot_learning_rate_curve(self):\n",
        "        \"\"\"Plota curva de learning rate separadamente\"\"\"\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        plt.plot(self.learning_rates, color='green', linewidth=2)\n",
        "        plt.title(f'Learning Rate Schedule - {self.scheduler_type}')\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.ylabel('Learning Rate')\n",
        "        plt.yscale('log')\n",
        "        plt.grid(True, alpha=0.3)\n",
        "        plt.savefig(f'{self.results_path}/learning_rate_curve.png', dpi=300, bbox_inches='tight')\n",
        "        plt.show()\n",
        "\n",
        "# ========================================================================================\n",
        "# üöÄ FUN√á√ÉO PRINCIPAL DE TREINAMENTO COM SCHEDULER\n",
        "# ========================================================================================\n",
        "\n",
        "def main_with_advanced_scheduler():\n",
        "    \"\"\"Fun√ß√£o principal com scheduler avan√ßado\"\"\"\n",
        "    \n",
        "    try:\n",
        "        print(\"üî• INICIANDO SISTEMA DE TREINAMENTO COM SCHEDULER AVAN√áADO\")\n",
        "        print(\"=\"*80)\n",
        "        \n",
        "        # Configura√ß√µes otimizadas\n",
        "        CONFIG = {\n",
        "            'BATCH_SIZE': 32,\n",
        "            'EPOCHS': 300,\n",
        "            'LEARNING_RATE': 0.001,  # LR inicial mais alto pois usaremos scheduler\n",
        "            'VALIDATION_SPLIT': 0.15,\n",
        "            'TARGET_SIZE': (224, 224),\n",
        "            'USE_AMP': True,\n",
        "            'NUM_WORKERS': 4,\n",
        "            'SCHEDULER_TYPE': 'cosine_with_warmup',  # Op√ß√µes: cosine_with_warmup, reduce_on_plateau, step_lr, exponential, cyclic, none\n",
        "            'PATIENCE': 30\n",
        "        }\n",
        "        \n",
        "        print(f\"üìã Configura√ß√µes:\")\n",
        "        for key, value in CONFIG.items():\n",
        "            print(f\"   ‚Ä¢ {key}: {value}\")\n",
        "        \n",
        "        # Caminhos\n",
        "        DATASET_PATH = \"/content/drive/MyDrive/GuilhermeAlmeida/fotos\"\n",
        "        LABELS_PATH = \"/content/drive/MyDrive/GuilhermeAlmeida/smart_labels.json\"\n",
        "        MODEL_SAVE_PATH = \"/content/drive/MyDrive/GuilhermeAlmeida/modelos/modelo_com_scheduler.pth\"\n",
        "        RESULTS_PATH = \"/content/drive/MyDrive/GuilhermeAlmeida/resultados\"\n",
        "        \n",
        "        print(f\"üìÇ Paths configurados\")\n",
        "        \n",
        "        # Dataset\n",
        "        print(\"üìÇ Carregando dataset...\")\n",
        "        full_dataset = AdvancedEyeTrackingDataset(\n",
        "            images_path=DATASET_PATH,\n",
        "            labels_path=LABELS_PATH,\n",
        "            target_size=CONFIG['TARGET_SIZE'],\n",
        "            augment=True\n",
        "        )\n",
        "        \n",
        "        # Split treino/valida√ß√£o\n",
        "        train_size = int((1 - CONFIG['VALIDATION_SPLIT']) * len(full_dataset))\n",
        "        val_size = len(full_dataset) - train_size\n",
        "        train_dataset, val_dataset = torch.utils.data.random_split(\n",
        "            full_dataset, [train_size, val_size]\n",
        "        )\n",
        "        \n",
        "        print(f\"üìä Dataset split: {train_size} treino, {val_size} valida√ß√£o\")\n",
        "        \n",
        "        # DataLoaders\n",
        "        train_loader = DataLoader(\n",
        "            train_dataset,\n",
        "            batch_size=CONFIG['BATCH_SIZE'],\n",
        "            shuffle=True,\n",
        "            num_workers=CONFIG['NUM_WORKERS'],\n",
        "            pin_memory=True,\n",
        "            drop_last=True\n",
        "        )\n",
        "        \n",
        "        val_loader = DataLoader(\n",
        "            val_dataset,\n",
        "            batch_size=CONFIG['BATCH_SIZE'],\n",
        "            shuffle=False,\n",
        "            num_workers=CONFIG['NUM_WORKERS'],\n",
        "            pin_memory=True\n",
        "        )\n",
        "        \n",
        "        print(f\"üìä DataLoaders criados\")\n",
        "        \n",
        "        # Modelo\n",
        "        print(\"üèóÔ∏è Criando modelo...\")\n",
        "        model = AdvancedEyeTrackingCNN(dropout_rate=0.3)\n",
        "        \n",
        "        # Conta par√¢metros\n",
        "        total_params = sum(p.numel() for p in model.parameters())\n",
        "        trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "        print(f\"üß† Modelo: {total_params:,} par√¢metros ({trainable_params:,} trein√°veis)\")\n",
        "        \n",
        "        # Trainer com scheduler\n",
        "        print(\"üöÄ Configurando trainer com scheduler avan√ßado...\")\n",
        "        trainer = AdvancedEyeTrackingTrainerWithScheduler(\n",
        "            model=model,\n",
        "            device=device,\n",
        "            use_amp=CONFIG['USE_AMP'],\n",
        "            save_path=MODEL_SAVE_PATH,\n",
        "            results_path=RESULTS_PATH\n",
        "        )\n",
        "        \n",
        "        # Teste r√°pido do modelo\n",
        "        print(\"üß™ Testando modelo...\")\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            test_batch = next(iter(train_loader))\n",
        "            test_data, test_target = test_batch\n",
        "            test_data, test_target = test_data.to(device), test_target.to(device)\n",
        "            \n",
        "            test_output = model(test_data)\n",
        "            test_loss = F.mse_loss(test_output, test_target)\n",
        "            \n",
        "            if torch.isnan(test_output).any() or torch.isnan(test_loss):\n",
        "                print(\"‚ùå Modelo produz valores inv√°lidos!\")\n",
        "                return\n",
        "            \n",
        "            print(f\"‚úÖ Modelo validado - Test Loss: {test_loss.item():.4f}\")\n",
        "        \n",
        "        # INICIA TREINAMENTO COM SCHEDULER\n",
        "        print(\"\\nüöÄ INICIANDO TREINAMENTO COM SCHEDULER AVAN√áADO...\")\n",
        "        start_time = time.time()\n",
        "        \n",
        "        train_losses, val_losses, train_accs, val_accs = trainer.train(\n",
        "            train_loader=train_loader,\n",
        "            val_loader=val_loader,\n",
        "            epochs=CONFIG['EPOCHS'],\n",
        "            learning_rate=CONFIG['LEARNING_RATE'],\n",
        "            scheduler_type=CONFIG['SCHEDULER_TYPE'],\n",
        "            patience=CONFIG['PATIENCE']\n",
        "        )\n",
        "        \n",
        "        training_time = time.time() - start_time\n",
        "        print(f\"\\n‚è±Ô∏è Tempo total de treinamento: {training_time/3600:.2f} horas\")\n",
        "        \n",
        "        print(\"\\nüéâ TREINAMENTO COM SCHEDULER CONCLU√çDO!\")\n",
        "        print(f\"üèÜ Melhor modelo salvo em: {MODEL_SAVE_PATH}\")\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Erro cr√≠tico: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "\n",
        "# ========================================================================================\n",
        "# üéØ EXECU√á√ÉO COM DIFERENTES SCHEDULERS\n",
        "# ========================================================================================\n",
        "\n",
        "print(\"üéØ SISTEMA DE TREINAMENTO COM SCHEDULER AVAN√áADO CARREGADO!\")\n",
        "print(\"\\nüìö Schedulers dispon√≠veis:\")\n",
        "print(\"   ‚Ä¢ cosine_with_warmup: Cosine annealing + warm-up (RECOMENDADO)\")\n",
        "print(\"   ‚Ä¢ reduce_on_plateau: Reduz LR quando loss estagna\")\n",
        "print(\"   ‚Ä¢ step_lr: Reduz LR em intervalos fixos\")\n",
        "print(\"   ‚Ä¢ exponential: Decay exponencial suave\")\n",
        "print(\"   ‚Ä¢ cyclic: LR c√≠clico para escape de m√≠nimos locais\")\n",
        "print(\"   ‚Ä¢ none: Learning rate fixo\")\n",
        "\n",
        "print(\"\\nüöÄ Para executar, chame: main_with_advanced_scheduler()\")\n",
        "print(\"üí° Dica: O scheduler 'cosine_with_warmup' √© o mais moderno e eficaz!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ========================================================================================\n",
        "# üöÄ EXECUTAR TREINAMENTO COM SCHEDULER AVAN√áADO\n",
        "# ========================================================================================\n",
        "\n",
        "# Execute esta c√©lula para iniciar o treinamento com scheduler\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"üî• INICIANDO TREINAMENTO COM SCHEDULER AVAN√áADO...\")\n",
        "    print(\"=\"*60)\n",
        "    \n",
        "    # Testa o sistema primeiro\n",
        "    if test_system():\n",
        "        print(\"\\n‚úÖ Sistema validado! Iniciando treinamento...\")\n",
        "        main_with_advanced_scheduler()\n",
        "    else:\n",
        "        print(\"\\n‚ùå Sistema n√£o passou nos testes b√°sicos!\")\n",
        "        print(\"üí° Verifique se todas as depend√™ncias est√£o instaladas corretamente.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ========================================================================================\n",
        "# üß™ TESTE R√ÅPIDO DO DATASET CORRIGIDO\n",
        "# ========================================================================================\n",
        "\n",
        "def test_dataset_fix():\n",
        "    \"\"\"Teste r√°pido para verificar se o erro de valida√ß√£o foi corrigido\"\"\"\n",
        "    print(\"üß™ TESTANDO CORRE√á√ÉO DO DATASET...\")\n",
        "    \n",
        "    # Simula dados problem√°ticos que estavam causando o erro\n",
        "    test_labels = [\n",
        "        [\"0.5\", \"0.3\"],  # Strings que causavam o erro\n",
        "        [0.7, 0.8],      # Floats normais\n",
        "        [\"0.2\", 0.4],    # Misto string/float\n",
        "        [1.2, 0.5],      # Fora do range\n",
        "        [\"invalid\", \"0.5\"],  # String inv√°lida\n",
        "        [0.3, 0.6]       # Float normal\n",
        "    ]\n",
        "    \n",
        "    print(f\"üìã Testando {len(test_labels)} labels problem√°ticos:\")\n",
        "    for i, label in enumerate(test_labels):\n",
        "        print(f\"   Label {i}: {label} (tipo: {type(label[0])}, {type(label[1])})\")\n",
        "    \n",
        "    # Simula o processo de convers√£o que seria feito no dataset\n",
        "    converted_labels = []\n",
        "    for i, label in enumerate(test_labels):\n",
        "        try:\n",
        "            if isinstance(label, (list, tuple)) and len(label) == 2:\n",
        "                x = float(label[0])\n",
        "                y = float(label[1])\n",
        "                # Verifica range\n",
        "                if 0 <= x <= 1 and 0 <= y <= 1:\n",
        "                    converted_labels.append([x, y])\n",
        "                    print(f\"   ‚úÖ Label {i}: {[x, y]} - OK\")\n",
        "                else:\n",
        "                    converted_labels.append([0.5, 0.5])\n",
        "                    print(f\"   ‚ö†Ô∏è Label {i}: Fora do range, usando [0.5, 0.5]\")\n",
        "            else:\n",
        "                converted_labels.append([0.5, 0.5])\n",
        "                print(f\"   ‚ö†Ô∏è Label {i}: Formato inv√°lido, usando [0.5, 0.5]\")\n",
        "        except (ValueError, TypeError) as e:\n",
        "            converted_labels.append([0.5, 0.5])\n",
        "            print(f\"   ‚ùå Label {i}: Erro {e}, usando [0.5, 0.5]\")\n",
        "    \n",
        "    print(f\"\\n‚úÖ TESTE CONCLU√çDO!\")\n",
        "    print(f\"üìä Labels convertidos: {len(converted_labels)}\")\n",
        "    print(f\"üìã Resultado final: {converted_labels}\")\n",
        "    \n",
        "    # Verifica se todos s√£o v√°lidos agora\n",
        "    all_valid = True\n",
        "    for i, label in enumerate(converted_labels):\n",
        "        x, y = label\n",
        "        if not (isinstance(x, (int, float)) and isinstance(y, (int, float))):\n",
        "            all_valid = False\n",
        "            print(f\"‚ùå Label {i} ainda tem problema de tipo\")\n",
        "        elif not (0 <= x <= 1 and 0 <= y <= 1):\n",
        "            all_valid = False\n",
        "            print(f\"‚ùå Label {i} ainda tem problema de range\")\n",
        "    \n",
        "    if all_valid:\n",
        "        print(\"üéâ TODOS OS LABELS AGORA S√ÉO V√ÅLIDOS!\")\n",
        "        return True\n",
        "    else:\n",
        "        print(\"‚ùå Ainda h√° labels problem√°ticos\")\n",
        "        return False\n",
        "\n",
        "# Executa o teste\n",
        "if test_dataset_fix():\n",
        "    print(\"\\n‚úÖ CORRE√á√ÉO VALIDADA! O dataset agora deve funcionar corretamente.\")\n",
        "    print(\"üöÄ Pode executar o treinamento sem o erro de compara√ß√£o string/int.\")\n",
        "else:\n",
        "    print(\"\\n‚ùå Ainda h√° problemas na corre√ß√£o.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üéõÔ∏è EXEMPLOS DE CONFIGURA√á√ÉO DE SCHEDULER\n",
            "==================================================\n",
            "\n",
            "üî• CONFIGURA√á√ÉO RECOMENDADA (Cosine with Warmup):\n",
            "CONFIG = {\n",
            "    'BATCH_SIZE': 32,\n",
            "    'EPOCHS': 300,\n",
            "    'LEARNING_RATE': 0.001,\n",
            "    'SCHEDULER_TYPE': 'cosine_with_warmup',  # üî• LEARNING RATE VARI√ÅVEL!\n",
            "    'PATIENCE': 30,\n",
            "}\n",
            "\n",
            "üéØ CONFIGURA√á√ÉO PARA FINE-TUNING (Reduce on Plateau):\n",
            "CONFIG = {\n",
            "    'BATCH_SIZE': 32,\n",
            "    'EPOCHS': 300,\n",
            "    'LEARNING_RATE': 0.0005,  # LR menor para fine-tuning\n",
            "    'SCHEDULER_TYPE': 'reduce_on_plateau',  # üìâ Adapta automaticamente\n",
            "    'PATIENCE': 30,\n",
            "}\n",
            "\n",
            "üß™ CONFIGURA√á√ÉO EXPERIMENTAL (Cyclic LR):\n",
            "CONFIG = {\n",
            "    'BATCH_SIZE': 32,\n",
            "    'EPOCHS': 300,\n",
            "    'LEARNING_RATE': 0.002,  # LR mais alto para cyclic\n",
            "    'SCHEDULER_TYPE': 'cyclic',  # üîÑ LR oscilante\n",
            "    'PATIENCE': 50,  # Paci√™ncia maior\n",
            "}\n",
            "\n",
            "==================================================\n",
            "üí° COMO USAR:\n",
            "1. Escolha uma configura√ß√£o acima\n",
            "2. Copie e cole na C√©lula 23 (main_with_advanced_scheduler)\n",
            "3. Substitua a vari√°vel CONFIG existente\n",
            "4. Execute o treinamento!\n",
            "\n",
            "üöÄ DICA: Comece com a configura√ß√£o RECOMENDADA!\n",
            "\n",
            "üìç LOCALIZA√á√ÉO:\n",
            "‚Ä¢ C√©lula 23: Fun√ß√£o 'main_with_advanced_scheduler()'\n",
            "‚Ä¢ Procure por: CONFIG = {\n",
            "‚Ä¢ Substitua a linha: 'SCHEDULER_TYPE': 'cosine_with_warmup'\n",
            "‚Ä¢ Por exemplo: 'SCHEDULER_TYPE': 'reduce_on_plateau'\n"
          ]
        }
      ],
      "source": [
        "# ========================================================================================\n",
        "# üéõÔ∏è EXEMPLO PR√ÅTICO: CONFIGURA√á√ïES DE SCHEDULER\n",
        "# ========================================================================================\n",
        "\n",
        "# Execute esta c√©lula para ver exemplos de diferentes configura√ß√µes\n",
        "\n",
        "print(\"üéõÔ∏è EXEMPLOS DE CONFIGURA√á√ÉO DE SCHEDULER\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# Configura√ß√£o 1: RECOMENDADA (Cosine with Warmup)\n",
        "config_recomendada = {\n",
        "    'BATCH_SIZE': 32,\n",
        "    'EPOCHS': 300,\n",
        "    'LEARNING_RATE': 0.001,    # LR inicial mais alto pois vai diminuir\n",
        "    'SCHEDULER_TYPE': 'cosine_with_warmup',  # üî• RECOMENDADO\n",
        "    'PATIENCE': 30\n",
        "}\n",
        "\n",
        "print(\"\\nüî• CONFIGURA√á√ÉO RECOMENDADA (Cosine with Warmup):\")\n",
        "print(\"CONFIG = {\")\n",
        "for key, value in config_recomendada.items():\n",
        "    if key == 'SCHEDULER_TYPE':\n",
        "        print(f\"    '{key}': '{value}',  # üî• LEARNING RATE VARI√ÅVEL!\")\n",
        "    else:\n",
        "        print(f\"    '{key}': {value},\")\n",
        "print(\"}\")\n",
        "\n",
        "# Configura√ß√£o 2: Fine-tuning (Reduce on Plateau)\n",
        "config_fine_tuning = {\n",
        "    'BATCH_SIZE': 32,\n",
        "    'EPOCHS': 300,\n",
        "    'LEARNING_RATE': 0.0005,   # LR inicial menor para fine-tuning\n",
        "    'SCHEDULER_TYPE': 'reduce_on_plateau',\n",
        "    'PATIENCE': 30\n",
        "}\n",
        "\n",
        "print(\"\\nüéØ CONFIGURA√á√ÉO PARA FINE-TUNING (Reduce on Plateau):\")\n",
        "print(\"CONFIG = {\")\n",
        "for key, value in config_fine_tuning.items():\n",
        "    if key == 'SCHEDULER_TYPE':\n",
        "        print(f\"    '{key}': '{value}',  # üìâ Adapta automaticamente\")\n",
        "    elif key == 'LEARNING_RATE':\n",
        "        print(f\"    '{key}': {value},  # LR menor para fine-tuning\")\n",
        "    else:\n",
        "        print(f\"    '{key}': {value},\")\n",
        "print(\"}\")\n",
        "\n",
        "# Configura√ß√£o 3: Experimental (Cyclic)\n",
        "config_experimental = {\n",
        "    'BATCH_SIZE': 32,\n",
        "    'EPOCHS': 300,\n",
        "    'LEARNING_RATE': 0.002,    # LR mais alto para cyclic\n",
        "    'SCHEDULER_TYPE': 'cyclic',\n",
        "    'PATIENCE': 50  # Paci√™ncia maior para permitir oscila√ß√µes\n",
        "}\n",
        "\n",
        "print(\"\\nüß™ CONFIGURA√á√ÉO EXPERIMENTAL (Cyclic LR):\")\n",
        "print(\"CONFIG = {\")\n",
        "for key, value in config_experimental.items():\n",
        "    if key == 'SCHEDULER_TYPE':\n",
        "        print(f\"    '{key}': '{value}',  # üîÑ LR oscilante\")\n",
        "    elif key == 'LEARNING_RATE':\n",
        "        print(f\"    '{key}': {value},  # LR mais alto para cyclic\")\n",
        "    elif key == 'PATIENCE':\n",
        "        print(f\"    '{key}': {value},  # Paci√™ncia maior\")\n",
        "    else:\n",
        "        print(f\"    '{key}': {value},\")\n",
        "print(\"}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"üí° COMO USAR:\")\n",
        "print(\"1. Escolha uma configura√ß√£o acima\")\n",
        "print(\"2. Copie e cole na C√©lula 23 (main_with_advanced_scheduler)\")\n",
        "print(\"3. Substitua a vari√°vel CONFIG existente\")\n",
        "print(\"4. Execute o treinamento!\")\n",
        "\n",
        "print(\"\\nüöÄ DICA: Comece com a configura√ß√£o RECOMENDADA!\")\n",
        "\n",
        "# Mostra onde encontrar a fun√ß√£o principal\n",
        "print(\"\\nüìç LOCALIZA√á√ÉO:\")\n",
        "print(\"‚Ä¢ C√©lula 23: Fun√ß√£o 'main_with_advanced_scheduler()'\")\n",
        "print(\"‚Ä¢ Procure por: CONFIG = {\")\n",
        "print(\"‚Ä¢ Substitua a linha: 'SCHEDULER_TYPE': 'cosine_with_warmup'\")\n",
        "print(\"‚Ä¢ Por exemplo: 'SCHEDULER_TYPE': 'reduce_on_plateau'\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
